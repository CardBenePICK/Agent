import os
import json
import time
import requests
from elasticsearch import Elasticsearch

# ================================
# ì„¤ì •
# ================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
INDEX_NAME = "card_benefit_bgem3_v1"
HF_TOKEN = os.getenv("HF_API_KEY") or os.getenv("HF_TOKEN")
ROUTER_API_URL = "https://router.huggingface.co/v1/chat/completions"
MODEL_NAME = "google/gemma-2-9b-it:nebius"

OUTPUT_JSON = "brand_dictionary.json"
CHECKPOINT_JSON = "brand_progress.json"

CHUNK_SIZE = 1          # ë¬¸ì„œë‹¹ 1 chunk
SAVE_EVERY = 20         # N chunks ì²˜ë¦¬í•  ë•Œë§ˆë‹¤ ìë™ ì €ì¥
RETRY_LIMIT = 3         # HF API ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜


# ================================
# HF Router ChatCompletion wrapper
# ================================
def hf_chat(messages):
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "max_tokens": 120,
    }

    for attempt in range(1, RETRY_LIMIT + 1):
        try:
            resp = requests.post(ROUTER_API_URL, json=payload, headers=headers, timeout=30)
            data = resp.json()
            return data
        except Exception as e:
            print(f"âš ï¸ HF API ì˜¤ë¥˜ (ì‹œë„ {attempt}/{RETRY_LIMIT}) â†’ {e}")
            time.sleep(2)

    print("âŒ HF API ì¬ì‹œë„ ì‹¤íŒ¨ â†’ None ë°˜í™˜")
    return None


# ================================
# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
# ================================
def load_checkpoint():
    if not os.path.exists(CHECKPOINT_JSON):
        return {"processed": 0}

    with open(CHECKPOINT_JSON, "r", encoding="utf-8") as f:
        return json.load(f)


def save_checkpoint(idx):
    with open(CHECKPOINT_JSON, "w", encoding="utf-8") as f:
        json.dump({"processed": idx}, f, ensure_ascii=False, indent=2)


# ================================
# ê¸°ì¡´ ë¸Œëœë“œ ì‚¬ì „ ëˆ„ì  ë¡œë“œ
# ================================
def load_brand_dict():
    if not os.path.exists(OUTPUT_JSON):
        return set()

    with open(OUTPUT_JSON, "r", encoding="utf-8") as f:
        old_list = json.load(f)
        return set(old_list)


def save_brand_dict(brand_set):
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(sorted(list(brand_set)), f, ensure_ascii=False, indent=2)


# ================================
# Elasticsearch ì „ì²´ ë¬¸ì„œ ë¡œë“œ
# ================================
def load_all_docs():
    print("ğŸ” Elasticsearch ì „ì²´ ë¬¸ì„œ ì½ëŠ” ì¤‘â€¦")

    es = Elasticsearch(ELASTICSEARCH_URL, verify_certs=False)
    query = {"query": {"match_all": {}}}

    docs = []
    res = es.search(index=INDEX_NAME, body=query, scroll="2m", size=500)
    sid = res["_scroll_id"]
    hits = res["hits"]["hits"]
    docs.extend(hits)

    while len(hits) > 0:
        res = es.scroll(scroll_id=sid, scroll="2m")
        sid = res["_scroll_id"]
        hits = res["hits"]["hits"]
        docs.extend(hits)

    print(f"ğŸ“„ ì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    return docs


# ================================
# ë¸Œëœë“œ ì¶”ì¶œ
# ================================
def extract_brands(text):
    messages = [
        {
            "role": "system",
            "content": "í…ìŠ¤íŠ¸ì—ì„œ ë¸Œëœë“œëª…ë§Œ JSON ë°°ì—´ë¡œ ì¶”ì¶œí•´ì¤˜. ì˜ˆ: ['ìŠ¤íƒ€ë²…ìŠ¤','ì´ë§ˆíŠ¸']"
        },
        {
            "role": "user",
            "content": text[:2000]  
        }
    ]

    res = hf_chat(messages)
    if not res or "choices" not in res:
        return []

    try:
        output = res["choices"][0]["message"]["content"]
        data = json.loads(output)
        return [x.strip() for x in data]
    except Exception:
        return []


# ================================
# ë©”ì¸ ì‹¤í–‰
# ================================
if __name__ == "__main__":

    # 1) ë°ì´í„° ë¡œë“œ
    docs = load_all_docs()
    total_chunks = len(docs)

    # 2) checkpoint & ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ
    checkpoint = load_checkpoint()
    start_idx = checkpoint["processed"]

    print(f"â³ ì´ì „ ì²˜ë¦¬ ì§€ì : chunk {start_idx}/{total_chunks}")
    brand_set = load_brand_dict()

    # 3) chunk ì²˜ë¦¬ loop
    print("\nğŸš€ ë¸Œëœë“œ ìë™ ì¶”ì¶œ ì‹œì‘ (ì¬ì‹œì‘ ê°€ëŠ¥)â€¦\n")

    for idx in range(start_idx, total_chunks):

        if idx % SAVE_EVERY == 0:
            print(f"ğŸ’¾ ìë™ ì €ì¥ â€” ì§„í–‰ë¥  {idx}/{total_chunks}")
            save_checkpoint(idx)
            save_brand_dict(brand_set)

        hit = docs[idx]["_source"]
        raw_text = hit.get("text", "")
        meta = hit.get("metadata", {})
        benefit = meta.get("benefit_summary", "")
        chunk_text = f"{benefit}\n{raw_text}"

        print(f"â†’ Chunk {idx}/{total_chunks} ì²˜ë¦¬ ì¤‘â€¦")

        try:
            brands = extract_brands(chunk_text)
            brand_set.update(brands)
        except Exception as e:
            print(f"âš ï¸ Chunk {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # 4) ìµœì¢… ì €ì¥
    print("\nğŸ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ â†’ ìµœì¢… ì €ì¥ ì¤‘â€¦")
    save_checkpoint(total_chunks)
    save_brand_dict(brand_set)

    print(f"ğŸ“¦ ë¸Œëœë“œ ì‚¬ì „ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_JSON}")
    print("âœ¨ ì‘ì—… ì¢…ë£Œ")
