import os
import json
from dotenv import load_dotenv
from tqdm import tqdm
from elasticsearch import Elasticsearch, helpers

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ============================================================
# 1. ì„¤ì • (Configuration)
# ============================================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")

# [ì„¤ì •] ë°ì´í„°ë¥¼ ì €ì¥í•  ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ì´ë¦„
INDEX_NAME = "credit_cards_nested_v2"

# [ì„¤ì •] ì´ë¯¸ ë²¡í„°ê°€ í¬í•¨ëœ ì›ë³¸ íŒŒì¼ ê²½ë¡œ (ì—…ë¡œë“œí•´ì£¼ì‹  íŒŒì¼ëª…)
INPUT_FILE = "credit_cards_backup.json" 

BATCH_SIZE = 100  # ë²¡í„° ìƒì„±ì´ ì—†ìœ¼ë¯€ë¡œ ë°°ì¹˜ë¥¼ í¬ê²Œ ì¡ìŒ

# ============================================================
# 2. Elasticsearch ì¸ë±ìŠ¤ ë° ë§µí•‘ ìƒì„±
# ============================================================
def create_index_if_not_exists(es: Elasticsearch):
    """
    ì¸ë±ìŠ¤ê°€ ì—†ì„ ê²½ìš°, Nested êµ¬ì¡°ì™€ Vector ì„¤ì •ì´ í¬í•¨ëœ ë§µí•‘ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "tokenizer": {
                    "nori_tokenizer_mixed": {
                        "type": "nori_tokenizer",
                        "decompound_mode": "mixed"
                    }
                },
                "analyzer": {
                    "korean_analyzer": {
                        "type": "custom",
                        "tokenizer": "nori_tokenizer_mixed",
                        "filter": ["lowercase", "nori_part_of_speech"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                # --- Root Fields ---
                "card_id": {"type": "keyword"},
                "card_name": {"type": "text", "analyzer": "korean_analyzer"},
                "card_company": {"type": "keyword"},
                "domestic_year_cost": {"type": "integer"},
                "abroad_year_cost": {"type": "integer"},
                "previous_month_performance": {"type": "integer"},
                
                # --- Nested Fields (Benefits) ---
                "benefits": {
                    "type": "nested", 
                    "properties": {
                        "category": {"type": "keyword"},
                        "summary": {"type": "text", "analyzer": "korean_analyzer"},
                        "description": {"type": "text", "analyzer": "korean_analyzer"},
                        
                        # [ì¤‘ìš”] ì´ë¯¸ ìˆëŠ” ë²¡í„°ë¥¼ ë‹´ì„ í•„ë“œ ì •ì˜ (ì°¨ì›ìˆ˜ 1024 í™•ì¸)
                        "vector": {
                            "type": "dense_vector",
                            "dims": 1024,  
                            "index": True,
                            "similarity": "cosine"
                        },
                        
                        # --- Nested Fields (Tiers) ---
                        "tiers": {
                            "type": "nested",
                            "properties": {
                                "previous_min_spend": {"type": "long"},
                                "rate": {"type": "float"},
                                "unit": {"type": "keyword"},
                                "type": {"type": "keyword"}
                            }
                        }
                    }
                }
            }
        }
    }

    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {INDEX_NAME}")
    else:
        # ë§µí•‘ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ì‚­ì œ í›„ ì¬ìƒì„± ì¶”ì²œ (ì„ íƒì‚¬í•­)
        print(f"â„¹ï¸ ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {INDEX_NAME}")
        # es.indices.delete(index=INDEX_NAME)
        # es.indices.create(index=INDEX_NAME, body=mapping)
        # print(f"â™»ï¸ ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ")

# ============================================================
# 3. ë°ì´í„° ë¡œë”© ë° ë³€í™˜ (Generator)
# ============================================================
def generate_actions(filename):
    """
    NDJSON íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ES Bulk Action í˜•íƒœë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    """
    with open(filename, 'r', encoding='utf-8') as f:
        for line_number, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            
            try:
                # 1. JSON íŒŒì‹±
                doc = json.loads(line)
                
                # 2. _source ë°ì´í„° ì¶”ì¶œ (ë°±ì—… íŒŒì¼ êµ¬ì¡°ì— ë”°ë¦„)
                source_data = doc.get('_source')
                if not source_data:
                    continue

                # 3. ë°ì´í„° ì •ì œ (í•„ìš”ì‹œ)
                # ì›ë³¸ íŒŒì¼ì— ì´ë¯¸ ë²¡í„°ê°€ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                
                # 4. Bulk Action ìƒì„±
                action = {
                    "_index": INDEX_NAME,
                    "_id": source_data.get("card_id"),  # ID ì§€ì •
                    "_source": source_data
                }
                yield action

            except json.JSONDecodeError:
                print(f"âš ï¸ JSON íŒŒì‹± ì—ëŸ¬ (Line {line_number})")
                continue

# ============================================================
# 4. ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INPUT_FILE}")

    # 1. ES ì—°ê²°
    es = Elasticsearch(ELASTICSEARCH_URL)
    if not es.ping():
        raise ConnectionError("âŒ Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2. ì¸ë±ìŠ¤ ì¤€ë¹„
    create_index_if_not_exists(es)

    # 3. ë°ì´í„° ì…ë ¥ (Bulk Insert)
    print(f"ğŸš€ ë°ì´í„° ì…ë ¥ì„ ì‹œì‘í•©ë‹ˆë‹¤... (File: {INPUT_FILE})")
    
    try:
        # helpers.streaming_bulkëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë„ ëŠì–´ì„œ ì „ì†¡í•¨
        success_count = 0
        failed_count = 0
        
        # ì§„í–‰ë¥  í‘œì‹œì¤„(tqdm)ê³¼ í•¨ê»˜ ì‹¤í–‰
        for ok, info in tqdm(helpers.streaming_bulk(es, generate_actions(INPUT_FILE), chunk_size=BATCH_SIZE)):
            if ok:
                success_count += 1
            else:
                failed_count += 1
                print(f"ì‹¤íŒ¨: {info}")

        print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
        print(f"âœ… ì„±ê³µ: {success_count} ê±´")
        if failed_count > 0:
            print(f"âŒ ì‹¤íŒ¨: {failed_count} ê±´")

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()