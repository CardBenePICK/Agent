import os
import sys
import requests
import json
import ast
import time
from typing import List, Dict, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€

# LangChain & LangSmith Imports
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from langchain_core.tools import tool
from langsmith import traceable
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY") or os.getenv("HF_TOKEN")
INDEX_NAME = "card_benefit_bgem3_v2"

# 1. Llama-3.1 (ì¶”ì²œ: JSON êµ¬ì¡°í™” ìš°ìˆ˜)
# MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct:novita" 

# 2. Gemma-2 (í•œêµ­ì–´ ë¬¸ë§¥ ì´í•´ ìš°ìˆ˜)
MODEL_NAME = "google/gemma-2-9b-it:nebius"

API_URL = "https://router.huggingface.co/v1/chat/completions"

# ============================================================
# 0. ì¹´í…Œê³ ë¦¬ ì‚¬ì „ ë¡œë“œ
# ============================================================

try:
    from category_dictionary import KNOWN_CATEGORIES
    print(f"âœ… ì¹´í…Œê³ ë¦¬ ì‚¬ì „ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ({len(KNOWN_CATEGORIES)}ê°œ í•­ëª©)")
except ImportError:
    print("âš ï¸ 'category_dictionary.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    KNOWN_CATEGORIES = []

# ============================================================
# 1. HF API Wrapper
# ============================================================

def query_hf_api(payload):
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json"
    }
    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        return response.json()
    except Exception as e:
        print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

# ============================================================
# 2. ìœ í‹¸ë¦¬í‹° (Safe Print)
# ============================================================

def safe_print(text):
    """ìœˆë„ìš° ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€ ì¶œë ¥ í•¨ìˆ˜"""
    try:
        print(text)
    except UnicodeEncodeError:
        try:
            print(text.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))
        except:
            pass

# ============================================================
# 3. í†µí•© ë¶„ì„ í•¨ìˆ˜ (One-Shot Extraction + ëŒ€ì†Œë¬¸ì í•´ê²°)
# ============================================================

@traceable(name="0. Analyze Query Unified", run_type="llm")
def analyze_query_unified(original_query: str) -> Dict:
    """
    LLM í˜¸ì¶œ í•œ ë²ˆìœ¼ë¡œ [ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬, í™•ì¥ê²€ìƒ‰ì–´]ë¥¼ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    
    # [ìˆ˜ì •] ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ 1ì°¨ ë§¤ì¹­ (ott -> OTT ì¸ì‹ í•´ê²°)
    found_categories = []
    query_lower = original_query.lower()
    for cat in KNOWN_CATEGORIES:
        if cat.lower() in query_lower:
            found_categories.append(cat)
    
    if not HF_API_KEY:
        return {
            "brands": [],
            "categories": list(set(found_categories)),
            "expanded_queries": [original_query]
        }

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    categories_str = ", ".join(KNOWN_CATEGORIES)

    messages = [
        {
            "role": "system",
            "content": "You are an expert in search query analysis. Extract information from the user's query and output it in JSON format."
        },
        {
            "role": "user",
            "content": f"""
ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ 3ê°€ì§€ ì •ë³´ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜.

1. **brands**: ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ë¸Œëœë“œë‚˜ ì„œë¹„ìŠ¤ëª… (ì˜ˆ: "ìŠ¤ë²…"->"ìŠ¤íƒ€ë²…ìŠ¤", "ë„·í”Œ"->"ë„·í”Œë¦­ìŠ¤"). ì¼ë°˜ ëª…ì‚¬ëŠ” ì œì™¸.
2. **categories**: ì•„ë˜ [ì¹´í…Œê³ ë¦¬ ëª©ë¡] ì¤‘ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ê²ƒë“¤. (ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ ì‚¬ìš© ê¸ˆì§€)
3. **expanded_queries**: ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í™•ì¥ ê²€ìƒ‰ì–´ **ìµœëŒ€ 3ê°œ**. (ë™ì˜ì–´, ì˜¤íƒˆì êµì • ë“±)

[ì¹´í…Œê³ ë¦¬ ëª©ë¡]
{categories_str}

[ì‚¬ìš©ì ì§ˆë¬¸]
"{original_query}"

[ì¶œë ¥ ì˜ˆì‹œ]
{{
  "brands": ["ìŠ¤íƒ€ë²…ìŠ¤", "ë„·í”Œë¦­ìŠ¤"],
  "categories": ["ì¹´í˜", "OTT/ì˜í™”/ë¬¸í™”"],
  "expanded_queries": ["ìŠ¤íƒ€ë²…ìŠ¤ í• ì¸ ì¹´ë“œ", "ë„·í”Œë¦­ìŠ¤ í˜œíƒ", "ì»¤í”¼ë¹ˆ í• ì¸"]
}}

ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´. ì„¤ëª…ì€ ìƒëµí•´.
"""
        }
    ]

    # API í˜¸ì¶œ
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "max_tokens": 500,
        "temperature": 0.1,
        "response_format": {"type": "json_object"} 
    }

    response_data = query_hf_api(payload)
    
    result = {
        "brands": [],
        "categories": list(set(found_categories)),
        "expanded_queries": [original_query]
    }

    if response_data and "choices" in response_data:
        content = response_data["choices"][0]["message"]["content"]
        
        try:
            clean_text = content.replace("```json", "").replace("```", "").strip()
            parsed = json.loads(clean_text)
            
            if "brands" in parsed and isinstance(parsed["brands"], list):
                result["brands"] = parsed["brands"]
            
            if "categories" in parsed and isinstance(parsed["categories"], list):
                valid_cats = [c for c in parsed["categories"] if c in KNOWN_CATEGORIES]
                result["categories"].extend(valid_cats)
                
            if "expanded_queries" in parsed and isinstance(parsed["expanded_queries"], list):
                # ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë§¨ ì•ì— ì¶”ê°€
                parsed["expanded_queries"].insert(0, original_query)
                result["expanded_queries"] = parsed["expanded_queries"]
                
        except json.JSONDecodeError:
            safe_print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ: {content}")
        except Exception as e:
            safe_print(f"âš ï¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì¤‘ë³µ ì œê±°
    result["brands"] = list(set(result["brands"]))
    result["categories"] = list(set(result["categories"]))
    result["expanded_queries"] = list(dict.fromkeys(result["expanded_queries"]))

    return result

# ============================================================
# 4. ê²€ìƒ‰ ë° ë­í‚¹ í—¬í¼ í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ & ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)
# ============================================================


def calculate_benefit_score(doc, user_brands: List[str], user_categories: List[str]) -> float:
    """
    í˜œíƒ ì ìˆ˜ ê³„ì‚° (ë©”íƒ€ë°ì´í„°ê°€ 0ì¼ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ìë™ ì¶”ì¶œ)
    """
    try:
        # 1. ë©”íƒ€ë°ì´í„°ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
        metadata_val = float(doc.metadata.get("benefit_value", 0))
        unit = doc.metadata.get("benefit_unit", "").strip()
        
        doc_cat = doc.metadata.get("category", "")
        summary = doc.metadata.get("benefit_summary", "")
        # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ (ì œëª© + ìš”ì•½ + ë³¸ë¬¸)
        text_all = (doc.metadata.get("card_name", "") + " " + 
                   summary + " " + 
                   (doc.page_content or ""))

        # 2. ì ìˆ˜ 1ì°¨ ì‚°ì • (ë©”íƒ€ë°ì´í„° ê¸°ì¤€)
        score = 0.0
        if unit == "%":
            score = metadata_val * 100 # 1% = 100ì 
        else:
            score = metadata_val # ì›í™”ëŠ” ê·¸ëŒ€ë¡œ

        # ğŸš€ [í•µì‹¬ ê°œì„ ] ë©”íƒ€ë°ì´í„° ì ìˆ˜ê°€ 0ì´ê±°ë‚˜ ë„ˆë¬´ ë‚®ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì±„êµ´
        if score < 10: 
            # (1) % íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: 10%, 5.5%)
            # \d+(?:\.\d+)? : ì •ìˆ˜ ë˜ëŠ” ì†Œìˆ˜
            pct_matches = re.findall(r'(\d+(?:\.\d+)?)\s*%', text_all)
            if pct_matches:
                # ì¶”ì¶œëœ % ê°’ ì¤‘ ìµœëŒ€ê°’ * 100
                max_pct = max([float(x) for x in pct_matches])
                score = max(score, max_pct * 100)

            # (2) ì›í™” íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: 1,500ì›, 1ë§Œì›, 20000 ì›)
            # ì½¤ë§ˆ(,) ì œê±° ë° 'ë§Œ' ë‹¨ìœ„ ì²˜ë¦¬ ë¡œì§ í•„ìš”
            # ê°„ë‹¨í•˜ê²Œ ìˆ«ì+ì› íŒ¨í„´ë§Œ ìš°ì„  ì²˜ë¦¬
            krw_matches = re.findall(r'(\d+(?:,\d+)*)\s*ì›', text_all)
            if krw_matches:
                # ì½¤ë§ˆ ì œê±° í›„ ìˆ«ìë¡œ ë³€í™˜
                amounts = [float(x.replace(',', '')) for x in krw_matches]
                if amounts:
                    score = max(score, max(amounts))
            
            # 'ë§Œì›' ë‹¨ìœ„ ì²˜ë¦¬ (ì˜ˆ: 1ë§Œì› -> 10000)
            man_matches = re.findall(r'(\d+(?:,\d+)*)\s*ë§Œì›', text_all)
            if man_matches:
                amounts_man = [float(x.replace(',', '')) * 10000 for x in man_matches]
                if amounts_man:
                    score = max(score, max(amounts_man))

        # ê¸°ë³¸ ì ìˆ˜ ë³´ì • (ì—¬ì „íˆ 0ì´ë©´ 1.0)
        if score <= 0: score = 1.0

        # 3. ê°€ì¤‘ì¹˜ ì ìš© (ë¸Œëœë“œ & ì¹´í…Œê³ ë¦¬)
        if user_brands and any(b in text_all for b in user_brands): 
            score *= 2.0
        
        if user_categories:
            # ì¹´í…Œê³ ë¦¬ëª… ì¼ì¹˜ ì‹œ
            if any(c in doc_cat or doc_cat in c for c in user_categories): 
                score *= 1.5
            # í…ìŠ¤íŠ¸ ë‚´ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë°œê²¬ ì‹œ
            elif any(c in text_all for c in user_categories): 
                score *= 1.2

        return score
    except Exception as e:
        # print(f"ì ìˆ˜ ê³„ì‚° ì—ëŸ¬: {e}")
        return 0.0

@traceable(name="1. Retrieve Candidates (Parallel)", run_type="retriever")
def retrieve_candidates(vector_store, queries: List[str], k_per_query: int):
    """
    [ì†ë„ ê°œì„ ] ë³‘ë ¬ ì²˜ë¦¬(Parallel Processing)ë¥¼ ì ìš©í•˜ì—¬ ê²€ìƒ‰ ì†ë„ë¥¼ ë†’ì„
    """
    all_docs = []
    # ì¤‘ë³µ/ë¹ˆ ì¿¼ë¦¬ ì œê±°
    unique_queries = list(set([q for q in queries if q.strip()]))

    def _single_search(query):
        try:
            return vector_store.similarity_search_with_score(query, k=k_per_query)
        except Exception as e:
            # print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    # ìµœëŒ€ 5ê°œì˜ ì“°ë ˆë“œë¡œ ë™ì‹œ ê²€ìƒ‰
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_query = {executor.submit(_single_search, q): q for q in unique_queries}
        
        for future in as_completed(future_to_query):
            docs_with_scores = future.result()
            for doc, score in docs_with_scores:
                if score < 0.15: continue 
                all_docs.append(doc)
    
    unique_docs = {doc.page_content: doc for doc in all_docs}.values()
    return list(unique_docs)

@traceable(name="2. Rerank Candidates", run_type="parser")
def rerank_candidates(docs: List, user_brands: List[str], user_categories: List[str]) -> List:
    docs_scored = []
    for doc in docs:
        score = calculate_benefit_score(doc, user_brands, user_categories)
        docs_scored.append((doc, score))
    
    sorted_docs = sorted(docs_scored, key=lambda x: x[1], reverse=True)
    return [d for d, s in sorted_docs]

@traceable(name="3. Select Top Results", run_type="parser")
def select_final_results(sorted_docs: List, top_k: int, user_brands: List[str], user_categories: List[str]) -> List[Dict]:
    top_results = []
    seen = set()

    for doc in sorted_docs:
        card_name = doc.metadata.get("card_name", "")
        if card_name in seen: continue
        seen.add(card_name)

        final_score = calculate_benefit_score(doc, user_brands, user_categories)
        top_results.append({
            "card_name": card_name,
            "origin_id": doc.metadata.get("origin_id", ""),
            "benefit_summary": doc.metadata.get("benefit_summary", ""),
            "score": final_score,
            "category": doc.metadata.get("category", ""),
            "match_reason": f"ë¸Œëœë“œ:{user_brands}, ì¹´í…Œê³ ë¦¬:{user_categories}"
        })
        if len(top_results) >= top_k: break
    return top_results

# ============================================================
# 5. Main Tool Definition
# ============================================================

@tool
def retriever_tool_unit_none(query: str) -> List[Dict]:
    """Search for credit card benefits using optimized unified analysis."""
    try:
        if not HF_API_KEY:
            safe_print("âŒ HF_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        embeddings = HuggingFaceEndpointEmbeddings(
            model="BAAI/bge-m3",
            task="feature-extraction",
            huggingfacehub_api_token=HF_API_KEY,
        )

        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL,
            index_name=INDEX_NAME,
            embedding=embeddings
        )

        # 1. í†µí•© ë¶„ì„
        analysis_result = analyze_query_unified(query)
        
        user_brands = analysis_result["brands"]
        user_categories = analysis_result["categories"]
        expanded_queries = analysis_result["expanded_queries"]
        
        safe_print(f"\nğŸ‘€ [ë¶„ì„ ê²°ê³¼ ({MODEL_NAME})]")
        safe_print(f"   - ë¸Œëœë“œ: {user_brands}")
        safe_print(f"   - ì¹´í…Œê³ ë¦¬: {user_categories}")
        safe_print(f"   - í™•ì¥ê²€ìƒ‰ì–´: {expanded_queries}")

        # 2. ë³‘ë ¬ ê²€ìƒ‰
        candidate_docs = retrieve_candidates(vector_store, expanded_queries, k_per_query=20)

        # 3. ë¦¬ë­í‚¹
        ranked_docs = rerank_candidates(candidate_docs, user_brands, user_categories)

        # 4. ê²°ê³¼ ì„ íƒ
        final_results = select_final_results(ranked_docs, top_k=3, user_brands=user_brands, user_categories=user_categories)

        return final_results

    except Exception as e:
        safe_print(f"âŒ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")
        return []

# ============================================================
# 6. ì‹¤í–‰ ë¸”ë¡
# ============================================================

if __name__ == "__main__":
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    if langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
        os.environ["LANGCHAIN_PROJECT"] = "CardBenefit RAG Debug"
        safe_print(f"âœ… LangSmith Tracing Enabled")
    else:
        safe_print("âš ï¸ LangSmith API Key not found.")

    safe_print(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Model: {MODEL_NAME})")
    safe_print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥\n")
    
    while True:
        try:
            user_query = input("\nğŸ’¬ ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
        except UnicodeDecodeError:
            continue
            
        if user_query.lower() in ["q", "quit"]:
            break
        if not user_query:
            continue

        # ğŸ•’ ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.perf_counter()

        results = retriever_tool_unit_none.invoke(user_query)

        # ğŸ•’ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time

        # â±ï¸ ì†Œìš” ì‹œê°„ ì¶œë ¥ (ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€)
        safe_print(f"\nâ±ï¸ [Total Time]: {elapsed_time:.4f} sec")

        if results:
            safe_print(f"ğŸ† [ì¶”ì²œ ê²°ê³¼ Top {len(results)}]")
            for i, res in enumerate(results):
                safe_print(f"{i+1}. {res['card_name']} (ì ìˆ˜: {res['score']:.1f})")
                safe_print(f"   - í˜œíƒ: {res['benefit_summary']}")
                safe_print(f"   - ë§¤ì¹­: {res['match_reason']}")
        else:
            safe_print("âš ï¸ ê²°ê³¼ ì—†ìŒ")