import os
import json
import time
# import re  # <-- ì •ê·œì‹ ëª¨ë“ˆ ë¶ˆí•„ìš”
import threading
from typing import List, Dict, Any
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv
from tqdm import tqdm

# [ì£¼ì˜] ì´ ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (HuggingFace API í‚¤ í•„ìš”)
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from elasticsearch import Elasticsearch, helpers

load_dotenv()

# ============================================================
# 1. ì„¤ì • (Configuration)
# ============================================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "credit_cards_nested_top100"  # êµ¬ì¡°ê°€ ë°”ê¼ˆìœ¼ë‹ˆ ì¸ë±ìŠ¤ ì´ë¦„ ë³€ê²½ ê¶Œì¥
INPUT_FILE = "processed_card_chunks_only_credit_1129.json"
CHECKPOINT_FILE = "ingest_nested_checkpoint_simple.json"
CARD_ID_TOP100_FILE = "card_ids_top100.json" # CARD_ID_TOP100 ë³€ìˆ˜ëª… ë³€ê²½ ë° íŒŒì¼ ì •ì˜
EMBEDDING_MODEL_ID = "BAAI/bge-m3"
EMBEDDING_DIM = 1024

BATCH_SIZE = 8
MAX_WORKERS = 4

# ============================================================
# 1-1. CARD_ID_TOP100 ëª©ë¡ ë¡œë“œ í•¨ìˆ˜ (ê°€ì •: íŒŒì¼ì— ID ë¦¬ìŠ¤íŠ¸ê°€ ìˆë‹¤ê³  ê°€ì •)
# ============================================================
def get_top_100_ids() -> List[str]:
    """
    ì‹¤ì œ í™˜ê²½ì—ì„œ CARD_ID_TOP100_FILE ë˜ëŠ” DB/APIì—ì„œ TOP 100 IDë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.
    
    ìš”ì²­í•˜ì‹  ID ëª©ë¡ì´ card_ids.jsonì— ìˆìœ¼ë¯€ë¡œ, ì´ íŒŒì¼ì„ ì½ëŠ” ê²ƒìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    (íŒŒì¼ëª…ì€ CARD_ID_TOP100_FILE ë³€ìˆ˜ë¡œ ì„¤ì •)
    """
    if os.path.exists(CARD_ID_TOP100_FILE):
        with open(CARD_ID_TOP100_FILE, 'r', encoding='utf-8') as f:
            try:
                # ì¹´ë“œ IDëŠ” ë¬¸ìì—´ ë˜ëŠ” ì •ìˆ˜ í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
                ids = json.load(f)
                return [str(i) for i in ids]
            except Exception as e:
                print(f"âš ï¸ {CARD_ID_TOP100_FILE} ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
                return []
    else:
        # [ì£¼ì˜] ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ì´ íŒŒì¼ ë˜ëŠ” DBì—ì„œ IDë¥¼ ë¡œë“œí•´ì•¼ í•¨
        print(f"âš ï¸ {CARD_ID_TOP100_FILE} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¸ë±ì‹± ëŒ€ìƒì„ ì œí•œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

# ============================================================
# 2. Elasticsearch ë§¤í•‘ (Schema)
# ... (ì´ ë¶€ë¶„ì€ ë™ì¼)
# ============================================================
def create_index_with_mapping(es: Elasticsearch):
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "korean_analyzer": {
                        "type": "nori",
                        "tokenizer": "nori_tokenizer"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                # --- Root Level ---
                "card_id": {"type": "keyword"},
                "card_name": {"type": "text", "analyzer": "korean_analyzer"},
                "card_company": {"type": "keyword"},
                "domestic_year_cost": {"type": "integer"},
                "abroad_year_cost": {"type": "integer"},
                "previous_month_performance": {"type": "integer"},
                
                # --- Nested Level ---
                "benefits": {
                    "type": "nested",
                    "properties": {
                        "category": {"type": "keyword"},
                        "summary": {"type": "text", "analyzer": "korean_analyzer"},
                        "description": {"type": "text", "analyzer": "korean_analyzer"},
                        
                        "vector": {
                            "type": "dense_vector",
                            "dims": EMBEDDING_DIM,
                            "index": True,
                            "similarity": "cosine"
                        },
                        
                        # [ìˆ˜ì •] ë‹¨ìˆœí™”ëœ Tiers êµ¬ì¡°
                        "tiers": {
                            "type": "nested",
                            "properties": {
                                "previous_min_spend": {"type": "long"},  # ì¹´ë“œì˜ ì „ì›”ì‹¤ì 
                                "rate": {"type": "float"},      # benefit_value
                                "unit": {"type": "keyword"},      # benefit_unit (%)
                                "type": {"type": "keyword"}      # benefit_type (saving/discount ë“±) [ì¶”ê°€ë¨]
                            }
                        }
                    }
                }
            }
        }
    }

    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {INDEX_NAME}")
    else:
        print(f"â„¹ï¸ ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {INDEX_NAME}")

# ============================================================
# 3. ë°ì´í„° êµ¬ì¡°í™” ë¡œì§ (Logic Changed)
# ============================================================
def create_tier_from_metadata(meta: Dict, card_min_spend: int) -> List[Dict]:
    """
    [ìˆ˜ì •ë¨] ë³µì¡í•œ í…ìŠ¤íŠ¸ íŒŒì‹± ì—†ì´ ë©”íƒ€ë°ì´í„° ê°’ì„ ê·¸ëŒ€ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.
    """
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    val = meta.get("benefit_value", 0)
    unit = meta.get("benefit_unit", "UNKNOWN")
    b_type = meta.get("benefit_type", "UNKNOWN")
    
    # --- [ì•ˆì „ì¥ì¹˜ ì¶”ê°€] ---
    try:
        rate = float(val)
    except (ValueError, TypeError):
        rate = 0.0
    # ----------------------

    # rate ê³„ì‚° (% ë‹¨ìœ„ ì²˜ë¦¬)
    if unit == "%" and rate > 0:
        rate = rate / 100.0

    return [{
        "previous_min_spend": card_min_spend,
        "rate": rate,
        "unit": unit,
        "type": b_type
    }]


# ============================================================
# 4. ë°ì´í„° ë³€í™˜ ë° ì„ë² ë”© ì²˜ë¦¬ (Worker)
# ============================================================
def process_single_card(card_id: str, chunks: List[Dict], embedding_model) -> Dict:
    if not chunks: return None

    try:
        base_meta = chunks[0]["metadata"]
        
        # Root í•„ë“œ ê°’
        domestic_cost = int(base_meta.get("domestic_year_cost", 0))
        abroad_cost = int(base_meta.get("abroad_year_cost", 0))
        prev_perf = int(base_meta.get("previous_month_performance", 0))
        
        card_doc = {
            "_id": card_id,
            "_index": INDEX_NAME,
            "_source": {
                "card_id": card_id,
                "card_name": base_meta.get("card_name", "Unknown"),
                "card_company": base_meta.get("card_company", ""),
                "domestic_year_cost": domestic_cost,
                "abroad_year_cost": abroad_cost,
                "previous_month_performance": prev_perf,
                "benefits": []
            }
        }

        for chunk in chunks:
            meta = chunk["metadata"]
            content = chunk["page_content"]
            
            # (A) [ë³€ê²½] íŒŒì‹± ëŒ€ì‹  ë©”íƒ€ë°ì´í„° ë§¤í•‘ í•¨ìˆ˜ í˜¸ì¶œ
            # ì¸ìë¡œ ì¹´ë“œì˜ ì „ì›”ì‹¤ì (prev_perf)ì„ ë„˜ê²¨ì„œ previous_min_spendë¡œ ì‚¬ìš©
            tiers = create_tier_from_metadata(meta, prev_perf)
            
            # (B) ì„ë² ë”© ìƒì„±
            text_to_embed = f"{meta.get('benefit_summary', '')} {content}"
            vector = embedding_model.embed_query(text_to_embed)
            
            # (C) í˜œíƒ ê°ì²´ ì¡°ë¦½
            benefit_obj = {
                "category": meta.get("category", "ê¸°íƒ€"),
                "summary": meta.get("benefit_summary", ""),
                "description": content,
                "vector": vector,
                "tiers": tiers
            }
            
            card_doc["_source"]["benefits"].append(benefit_obj)
            
        return card_doc

    except Exception as e:
        print(f"âš ï¸ ì¹´ë“œ {card_id} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        return None

# ============================================================
# 5. ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ============================================================
def main():
    if not HF_API_KEY:
        raise ValueError("HF_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    es = Elasticsearch(ELASTICSEARCH_URL)
    if not es.ping():
        raise ConnectionError("Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    create_index_with_mapping(es)

    print(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBEDDING_MODEL_ID}")
    embeddings = HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_MODEL_ID,
        task="feature-extraction",
        huggingfacehub_api_token=HF_API_KEY,
    )

    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"{INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    # ì „ì²´ ì¹´ë“œ ë°ì´í„°ë¥¼ card_idë³„ë¡œ ê·¸ë£¹í™”
    card_groups = defaultdict(list)
    for item in raw_data:
        cid = item["metadata"].get("card_id")
        if cid:
            card_groups[cid].append(item)
    
    # --- [ì¶”ê°€/ë³€ê²½] ì¸ë±ì‹± ëŒ€ìƒ ID ëª©ë¡ ë¡œë“œ ---
    top_100_ids = get_top_100_ids()
    if top_100_ids:
        print(f"âœ… TOP 100 ID ëª©ë¡ ë¡œë“œ ì™„ë£Œ. ({len(top_100_ids)}ê°œ)")
        top_100_set = set(top_100_ids)
    else:
        # íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ë©´ ëª¨ë“  ì¹´ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ í•¨ (ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬)
        top_100_set = set(card_groups.keys())
        print("âš ï¸ TOP 100 ID ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨. ëª¨ë“  ì¹´ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¸ë±ì‹±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

    # --- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë° ì‘ì—… ëŒ€ìƒ í•„í„°ë§ ---
    processed_ids = set()
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            processed_ids = set(json.load(f))
            
    # [ë³€ê²½] top_100_setì— í¬í•¨ë˜ì–´ ìˆê³ , ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ IDë§Œ ì„ íƒ
    target_card_ids = [
        cid for cid in card_groups.keys() 
        if cid in top_100_set and cid not in processed_ids
    ]
    
    # ì‘ì—… ëŒ€ìƒ ê·¸ë£¹ ìƒì„±
    target_groups = [(cid, card_groups[cid]) for cid in target_card_ids]
    
    # í…ŒìŠ¤íŠ¸ìš© 10ê°œ ì œí•œ (í•„ìš”ì‹œ ì£¼ì„ ì²˜ë¦¬)
    # target_groups = target_groups[:10]

    print(f"ğŸ“Š ì „ì²´ ì¹´ë“œ: {len(card_groups)}ê°œ | TOP 100 ëŒ€ìƒ: {len(top_100_set)}ê°œ | ì™„ë£Œ: {len(processed_ids)}ê°œ | ìµœì¢… ì‘ì—… ì˜ˆì •: {len(target_groups)}ê°œ")

    if not target_groups:
        print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ì¸ë±ì‹±í•  ëŒ€ìƒ ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    batches = [target_groups[i:i + BATCH_SIZE] for i in range(0, len(target_groups), BATCH_SIZE)]
    total_indexed = 0
    
    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (Workers: {MAX_WORKERS}, Batch Size: {BATCH_SIZE})")
    
    # ... (Bulk ì¸ë±ì‹± ë¡œì§ì€ ë™ì¼)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for batch in tqdm(batches, desc="Total Progress"):
            futures = []
            for cid, chunks in batch:
                futures.append(executor.submit(process_single_card, cid, chunks, embeddings))
            
            bulk_docs = []
            completed_ids = []
            
            for future in as_completed(futures):
                result_doc = future.result()
                if result_doc:
                    bulk_docs.append(result_doc)
                    completed_ids.append(result_doc["_source"]["card_id"])
            
            if bulk_docs:
                try:
                    success, failed = helpers.bulk(es, bulk_docs, stats_only=True)
                    if failed:
                        print(f"\nâš ï¸ {failed}ê±´ ì¸ë±ì‹± ì‹¤íŒ¨")
                    
                    processed_ids.update(completed_ids)
                    with open(CHECKPOINT_FILE, 'w') as f:
                        json.dump(list(processed_ids), f)
                        
                    total_indexed += len(completed_ids)

                except Exception as e:
                    print(f"\nâŒ ES Bulk Error: {e}")

    print(f"\nğŸ‰ ì‘ì—… ì¢…ë£Œ! ì´ {total_indexed}ê°œì˜ ì¹´ë“œê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
    # [ì£¼ì˜] ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— 'card_ids_top100.json' íŒŒì¼ì— ì¸ë±ì‹±í•  ID ëª©ë¡ì´ 
    # JSON ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: ["13", "51", ...])
    pass