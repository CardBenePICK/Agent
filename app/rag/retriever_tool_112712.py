import os
import sys
import requests
import json
import time
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache

# LangChain Imports
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain_elasticsearch import ElasticsearchStore
from langchain_core.tools import tool
from langsmith import traceable
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY") or os.getenv("HF_TOKEN")
INDEX_NAME = "card_benefit_bgem3_v2"

# 1. LLM ëª¨ë¸ ì„ íƒ
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct:novita" 

# 2. ì„ë² ë”© ëª¨ë¸ êµì²´ (API ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì¡°ì¹˜)
# BAAI/bge-m3 APIê°€ í˜„ì¬ ë¶ˆì•ˆì •í•˜ì—¬, ë™ì¼í•œ 1024ì°¨ì› ëª¨ë¸ì¸ e5-largeë¡œ êµì²´í•©ë‹ˆë‹¤.
EMBEDDING_MODEL_ID = "intfloat/multilingual-e5-large"

# ============================================================
# 0. ì¹´í…Œê³ ë¦¬ ì‚¬ì „ ë¡œë“œ
# ============================================================

try:
    from category_dictionary import KNOWN_CATEGORIES
    print(f"âœ… ì¹´í…Œê³ ë¦¬ ì‚¬ì „ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ({len(KNOWN_CATEGORIES)}ê°œ í•­ëª©)")
except ImportError:
    print("âš ï¸ 'category_dictionary.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    KNOWN_CATEGORIES = []

# ============================================================
# 1. Custom Embedding Class (URL ìˆ˜ì •ë¨ ğŸš€)
# ============================================================

class CustomHFEmbeddings(Embeddings):
    def __init__(self, api_key, model_id):
        self.api_key = api_key
        # ğŸš¨ [ìˆ˜ì •ë¨] Router URL í˜•ì‹ ì ìš©
        self.api_url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
        self.headers = {"Authorization": f"Bearer {api_key}"}

    def _query(self, texts: List[str]) -> Any:
        try:
            response = requests.post(
                self.api_url, 
                headers=self.headers, 
                json={"inputs": texts, "options": {"wait_for_model": True}}
            )
            return response.json()
        except Exception as e:
            print(f"âŒ HuggingFace API Connection Error: {e}")
            return []

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = self._query(texts)
        if isinstance(result, list):
            return result
        return []

    @lru_cache(maxsize=1000)
    def embed_query(self, text: str) -> List[float]:
        # e5 ëª¨ë¸ì€ query ì•ì— 'query: ' ì ‘ë‘ì–´ë¥¼ ë¶™ì´ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ì¢‹ìŒ
        if "e5" in EMBEDDING_MODEL_ID:
            text = f"query: {text}"
            
        result = self._query([text])
        
        # ê²°ê³¼ ê²€ì¦ ë° ì—ëŸ¬ í•¸ë“¤ë§
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list): 
                return result[0]
            elif isinstance(result[0], float): 
                return result
        
        # ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        if isinstance(result, dict) and 'error' in result:
            print(f"\nâŒ [Embedding Failed] API Error: {result['error']}")
        
        return []

# ============================================================
# 2. ìœ í‹¸ë¦¬í‹°
# ============================================================

def safe_print(text):
    try:
        print(text)
    except UnicodeEncodeError:
        try:
            print(text.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))
        except:
            pass

def query_hf_chat_api(payload):
    # LLMìš© Router URL
    chat_url = "https://router.huggingface.co/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(chat_url, headers=headers, json=payload)
        return response.json()
    except Exception as e:
        print(f"âŒ Chat API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

# ============================================================
# 3. í†µí•© ë¶„ì„ í•¨ìˆ˜ (LLM)
# ============================================================

@traceable(name="0. Analyze Query Unified", run_type="llm")
def analyze_query_unified(original_query: str) -> Dict:
    found_categories = []
    query_lower = original_query.lower()
    for cat in KNOWN_CATEGORIES:
        if cat.lower() in query_lower:
            found_categories.append(cat)
    
    if not HF_API_KEY:
        return {"brands": [], "categories": list(set(found_categories)), "expanded_queries": [original_query]}

    categories_str = ", ".join(KNOWN_CATEGORIES)
    messages = [
        {
            "role": "system",
            "content": "You are an expert in search query analysis. Extract information from the user's query and output it in JSON format."
        },
        {
            "role": "user",
            "content": f"""
ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ 3ê°€ì§€ ì •ë³´ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜.

1. **brands**: ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ë¸Œëœë“œë‚˜ ì„œë¹„ìŠ¤ëª… (ì˜ˆ: "ìŠ¤ë²…"->"ìŠ¤íƒ€ë²…ìŠ¤", "í†¡í†¡O"). ì¼ë°˜ ëª…ì‚¬ ì œì™¸.
2. **categories**: ì•„ë˜ [ì¹´í…Œê³ ë¦¬ ëª©ë¡] ì¤‘ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ê²ƒë“¤.
3. **expanded_queries**: ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í™•ì¥ ê²€ìƒ‰ì–´ **ìµœëŒ€ 3ê°œ**.

[ì¹´í…Œê³ ë¦¬ ëª©ë¡]
{categories_str}

[ì‚¬ìš©ì ì§ˆë¬¸]
"{original_query}"

[ì¶œë ¥ ì˜ˆì‹œ]
{{
  "brands": ["ìŠ¤íƒ€ë²…ìŠ¤"],
  "categories": ["ì¹´í˜"],
  "expanded_queries": ["ìŠ¤íƒ€ë²…ìŠ¤ í• ì¸ ì¹´ë“œ", "ì¹´í˜ í˜œíƒ"]
}}

ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´.
"""
        }
    ]

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "max_tokens": 500,
        "temperature": 0.1,
        "response_format": {"type": "json_object"}
    }

    response_data = query_hf_chat_api(payload)
    
    result = {
        "brands": [],
        "categories": list(set(found_categories)),
        "expanded_queries": [original_query]
    }

    if response_data and "choices" in response_data:
        try:
            content = response_data["choices"][0]["message"]["content"]
            clean_text = content.replace("```json", "").replace("```", "").strip()
            parsed = json.loads(clean_text)
            
            if "brands" in parsed: result["brands"] = parsed["brands"]
            if "categories" in parsed: 
                valid_cats = [c for c in parsed["categories"] if c in KNOWN_CATEGORIES]
                result["categories"].extend(valid_cats)
            if "expanded_queries" in parsed:
                parsed["expanded_queries"].insert(0, original_query)
                result["expanded_queries"] = parsed["expanded_queries"]
        except:
            pass

    result["brands"] = list(set(result["brands"]))
    result["categories"] = list(set(result["categories"]))
    result["expanded_queries"] = list(dict.fromkeys(result["expanded_queries"]))

    return result

# ============================================================
# 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ES Client ì§ì ‘ í˜¸ì¶œ)
# ============================================================

def perform_hybrid_search(vector_store, query: str, k: int, categories: List[str]) -> List[Any]:
    """
    Elasticsearch Clientë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ num_candidates íŒŒë¼ë¯¸í„°ë¥¼ í™•ì‹¤í•˜ê²Œ ì „ë‹¬
    """
    
    # 1. ì¿¼ë¦¬ ì„ë² ë”©
    query_vector = vector_store.embedding.embed_query(query)
    
    # ì„ë² ë”© ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not query_vector:
        return []
    
    # 2. ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±
    es_filter = []
    if categories:
        es_filter.append({"terms": {"metadata.category.keyword": categories}})

    # 3. ì•ˆì „í•œ í›„ë³´êµ° ìˆ˜ ì„¤ì • (kë³´ë‹¤ ì»¤ì•¼ í•¨)
    safe_num_candidates = max(100, k * 2)

    # 4. Raw Query Body ìƒì„±
    knn_query = {
        "field": "vector", 
        "query_vector": query_vector,
        "k": k,
        "num_candidates": safe_num_candidates,
        "filter": es_filter
    }
    
    body = {
        "knn": knn_query,
        "_source": ["text", "metadata", "page_content"] 
    }

    try:
        # 5. Client ì§ì ‘ í˜¸ì¶œ
        response = vector_store.client.search(index=INDEX_NAME, body=body)
        
        # 6. ê²°ê³¼ íŒŒì‹±
        results = []
        for hit in response["hits"]["hits"]:
            score = hit["_score"]
            source = hit["_source"]
            
            content = source.get("text") or source.get("page_content") or ""
            metadata = source.get("metadata", {})
            
            doc = Document(page_content=content, metadata=metadata)
            results.append((doc, score))
            
        return results

    except Exception as e:
        # print(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# ============================================================
# 5. ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹ (Parent Document Merge)
# ============================================================

def calculate_benefit_score(doc, user_brands, user_categories):
    try:
        base = float(doc.metadata.get("benefit_value", 0))
        unit = doc.metadata.get("benefit_unit", "").strip()
        doc_cat = doc.metadata.get("category", "")
        text = (doc.page_content or "") + " " + doc.metadata.get("benefit_summary", "")

        if unit == "%": score = base * 100
        else: score = base
        if score <= 0: score = 1.0

        if user_brands and any(b in text for b in user_brands): score *= 2.0
        
        if user_categories:
            if any(c in doc_cat or doc_cat in c for c in user_categories): score *= 1.5
            elif any(c in text for c in user_categories): score *= 1.2

        return score
    except:
        return 0.0

@traceable(name="1. Retrieve Candidates (Parallel)", run_type="retriever")
def retrieve_candidates(vector_store, queries: List[str], k_per_query: int, categories: List[str]):
    all_docs = []
    unique_queries = list(set([q for q in queries if q.strip()]))

    def _single_search(query):
        return perform_hybrid_search(vector_store, query, k_per_query, categories)

    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_query = {executor.submit(_single_search, q): q for q in unique_queries}
        
        for future in as_completed(future_to_query):
            results = future.result()
            for doc, score in results:
                if score < 0.1: continue 
                all_docs.append(doc)
    
    return list({doc.page_content: doc for doc in all_docs}.values())

@traceable(name="2. Rerank Candidates", run_type="parser")
def rerank_candidates(docs, user_brands, user_categories):
    docs_scored = []
    for doc in docs:
        score = calculate_benefit_score(doc, user_brands, user_categories)
        docs_scored.append((doc, score))
    return sorted(docs_scored, key=lambda x: x[1], reverse=True)

def select_final_results_with_merge(sorted_docs, top_k, user_brands, user_categories):
    merged_results = {} 

    for doc, score in sorted_docs:
        card_name = doc.metadata.get("card_name", "")
        if not card_name: continue

        summary = doc.metadata.get("benefit_summary", "")
        cat = doc.metadata.get("category", "")
        
        if card_name not in merged_results:
            merged_results[card_name] = {
                "card_name": card_name,
                "origin_id": doc.metadata.get("origin_id", ""),
                "summaries": [summary],
                "categories": [cat],
                "total_score": score,
                "match_reason": f"ë¸Œëœë“œ:{user_brands}, ì¹´í…Œê³ ë¦¬:{user_categories}"
            }
        else:
            if summary not in merged_results[card_name]["summaries"]:
                merged_results[card_name]["summaries"].append(summary)
            if cat not in merged_results[card_name]["categories"]:
                merged_results[card_name]["categories"].append(cat)
            
            merged_results[card_name]["total_score"] += (score * 0.1)

    final_list = []
    for info in merged_results.values():
        combined_summary = " / ".join(info["summaries"])
        combined_cat = ", ".join(list(set(info["categories"])))
        
        final_list.append({
            "card_name": info["card_name"],
            "score": info["total_score"],
            "benefit_summary": combined_summary,
            "category": combined_cat,
            "match_reason": info["match_reason"]
        })

    final_list = sorted(final_list, key=lambda x: x["score"], reverse=True)
    return final_list[:top_k]

# ============================================================
# 6. Main Tool
# ============================================================

@tool
def retriever_tool(query: str) -> List[Dict]:
    """Search for credit card benefits using Hybrid Search & Metadata Filtering."""
    try:
        if not HF_API_KEY:
            safe_print("âŒ HF_API_KEY ì—†ìŒ")
            return []

        embeddings = CustomHFEmbeddings(api_key=HF_API_KEY, model_id=EMBEDDING_MODEL_ID)

        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL,
            index_name=INDEX_NAME,
            embedding=embeddings
        )

        analysis = analyze_query_unified(query)
        user_brands = analysis["brands"]
        user_cats = analysis["categories"]
        queries = analysis["expanded_queries"]
        
        safe_print(f"\nğŸ‘€ [ë¶„ì„] ë¸Œëœë“œ:{user_brands}, ì¹´í…Œê³ ë¦¬:{user_cats}")
        safe_print(f"   [í™•ì¥ê²€ìƒ‰ì–´]: {queries}")

        candidates = retrieve_candidates(vector_store, queries, 60, user_cats)
        safe_print(f"   ğŸ‘‰ í›„ë³´êµ°: {len(candidates)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

        ranked = rerank_candidates(candidates, user_brands, user_cats)

        return select_final_results_with_merge(ranked, 3, user_brands, user_cats)

    except Exception as e:
        safe_print(f"âŒ ì˜¤ë¥˜: {e}")
        return []

# ============================================================
# 7. ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    # langsmith ì¶”ì  í™œì„±í™”
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    if langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
        os.environ["LANGCHAIN_PROJECT"] = "CardBenefit RAG Debug"
        safe_print(f"âœ… LangSmith Tracing Enabled")
    else:
        safe_print("âš ï¸ LangSmith API Key not found.")
    # safe print
    safe_print(f"ğŸ” ìµœì¢… ê³ ë„í™” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Model: {MODEL_NAME})")
    safe_print(f"   - Embed Model: {EMBEDDING_MODEL_ID}")
    safe_print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥ ë˜ëŠ” Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.\n")
    
    while True:
        try:
            q = input("\nğŸ’¬ ì…ë ¥: ").strip()
        except KeyboardInterrupt:
            safe_print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except EOFError:
            break
        except Exception:
            continue

        if q.lower() in ["q", "quit", "exit"]:
            safe_print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not q:
            continue

        start = time.perf_counter()
        results = retriever_tool.invoke(q)
        elapsed = time.perf_counter() - start

        safe_print(f"\nâ±ï¸ Time: {elapsed:.4f}s")

        if results:
            for i, res in enumerate(results):
                safe_print(f"{i+1}. {res['card_name']} ({res['score']:.0f}ì )")
                cat_str = res.get('category', 'ì¹´í…Œê³ ë¦¬ ì—†ìŒ')
                safe_print(f"   - ì¹´í…Œê³ ë¦¬: {cat_str}")
                safe_print(f"   - í˜œíƒ: {res['benefit_summary']}")
        else:
            safe_print("âš ï¸ ê²°ê³¼ ì—†ìŒ")