import os
import json
import time
import threading
from typing import List, Set
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

load_dotenv()

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "card_benefit_bgem3_v2"
CHECKPOINT_FILE = "ingest_checkpoint.json"
checkpoint_lock = threading.Lock()

# --- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ---
def load_completed_batches() -> Set[int]:
    if not os.path.exists(CHECKPOINT_FILE):
        return set()
    try:
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return set(data.get("completed_batches", []))
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ ì‹œì‘): {e}")
        return set()

def mark_batch_complete(batch_idx: int):
    with checkpoint_lock:
        completed = load_completed_batches()
        completed.add(batch_idx)
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump({"completed_batches": list(completed)}, f)

def load_processed_docs(json_path: str) -> List[Document]:
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        documents = []
        for item in data:
            doc = Document(page_content=item["page_content"], metadata=item["metadata"])
            documents.append(doc)
        print(f"âœ… JSONì—ì„œ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def ingest_batch(vector_store, batch, batch_idx):
    max_retries = 5
    last_error = None

    for attempt in range(max_retries):
        try:
            vector_store.add_documents(batch)
            mark_batch_complete(batch_idx)
            return True, batch_idx, None
            
        except Exception as e:
            last_error = e
            wait_time = (attempt + 1) * 5
            time.sleep(wait_time)
    
    return False, batch_idx, last_error

# ğŸ› ï¸ ìˆ˜ì •ë¨: batch_sizeì™€ max_workersë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
def ingest_documents_parallel(docs: List[Document], batch_size: int = 8, max_workers: int = 2):
    if not HF_API_KEY:
        raise ValueError("HF_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print(f"ğŸš€ Hugging Face API ì—°ê²° ì¤‘... (Model: BAAI/bge-m3)")
    
    embeddings = HuggingFaceEndpointEmbeddings(
        model="BAAI/bge-m3",
        task="feature-extraction",
        huggingfacehub_api_token=HF_API_KEY,
    )
    
    vector_store = ElasticsearchStore(
        es_url=ELASTICSEARCH_URL,
        index_name=INDEX_NAME,
        embedding=embeddings
    )
    
    # ì¸ìë¡œ ë°›ì€ batch_size ì‚¬ìš©
    all_batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]
    total_batches_count = len(all_batches)
    
    completed_batches = load_completed_batches()
    
    batches_to_process = []
    for i, batch in enumerate(all_batches):
        if i not in completed_batches:
            batches_to_process.append((i, batch))
    
    skipped_count = total_batches_count - len(batches_to_process)
    
    print("-" * 50)
    print(f"ğŸ“Š ì‘ì—… ìš”ì•½")
    print(f"   - ì„¤ì •: Batch Size={batch_size}, Max Workers={max_workers}")
    print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {total_batches_count}")
    print(f"   - ì™„ë£Œëœ ë°°ì¹˜: {skipped_count} (ê±´ë„ˆëœ€ âœ…)")
    print(f"   - ë‚¨ì€ ë°°ì¹˜  : {len(batches_to_process)} (ì‘ì—… ì˜ˆì • ğŸš€)")
    print("-" * 50)

    if not batches_to_process:
        print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì´ë¯¸ ì™„ë£Œë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return

    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (Max Workers: {max_workers})")

    # ğŸ› ï¸ ìˆ˜ì •ë¨: ì¸ìë¡œ ë°›ì€ max_workers ì‚¬ìš©
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(ingest_batch, vector_store, batch, idx) for idx, batch in batches_to_process]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="âš¡ ì„ë² ë”© ì ì¬ ì¤‘", unit="batch"):
            success, idx, err_msg = future.result()
            
            if not success:
                print(f"\nâŒ ë°°ì¹˜ {idx} ìµœì¢… ì‹¤íŒ¨! ì›ì¸: {err_msg}")

    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    JSON_FILE_PATH = os.path.join(current_dir, "processed_card_chunks_only_credit_1126.json")
    
    if os.path.exists(JSON_FILE_PATH):
        all_docs = load_processed_docs(JSON_FILE_PATH)
        target_docs = all_docs 
        
        # ğŸ› ï¸ ìˆ˜ì •ë¨: ì—¬ê¸°ì„œ ì•ˆì „í•œ ì„¤ì •ê°’(8, 2)ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        ingest_documents_parallel(target_docs, batch_size=8, max_workers=4)
    else:
        print(f"âš ï¸ '{JSON_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")