import os
import json
import time
import re
import threading
from typing import List, Dict, Any
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv
from tqdm import tqdm

# LangChain & Elasticsearch imports
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from elasticsearch import Elasticsearch, helpers

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ============================================================
# 1. ì„¤ì • (Configuration)
# ============================================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "credit_cards_nested_v1"  # ì¸ë±ìŠ¤ ë²„ì „ ì—…
INPUT_FILE = "processed_card_chunks_only_credit_1129.json"
CHECKPOINT_FILE = "ingest_nested_checkpoint_v2.json"

# ì„ë² ë”© ì„¤ì •
EMBEDDING_MODEL_ID = "BAAI/bge-m3"
EMBEDDING_DIM = 1024

# ë°°ì¹˜ ë° ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
BATCH_SIZE = 8       # í•œ ë²ˆì— ì²˜ë¦¬í•  'ì¹´ë“œ' ê°œìˆ˜
MAX_WORKERS = 4      # ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜

# ============================================================
# 2. Elasticsearch ë§¤í•‘ (Nested Schema)
# ============================================================
def create_index_with_mapping(es: Elasticsearch):
    """
    Card(Root) -> Benefits(Nested) êµ¬ì¡°ì˜ ì¸ë±ìŠ¤ ìƒì„±
    """
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "korean_analyzer": {
                        "type": "nori",
                        "tokenizer": "nori_tokenizer"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                # --- Root Level: ì¹´ë“œ ê¸°ë³¸ ì •ë³´ ---
                "card_id": {"type": "keyword"},
                "card_name": {"type": "text", "analyzer": "korean_analyzer"},
                "card_company": {"type": "keyword"},
                "annual_fee": {"type": "integer"},
                
                # --- Nested Level: í˜œíƒ ì •ë³´ ---
                "benefits": {
                    "type": "nested",  # ì¤‘ìš”: Nested íƒ€ì…
                    "properties": {
                        "category": {"type": "keyword"},
                        "summary": {"type": "text", "analyzer": "korean_analyzer"},
                        "description": {"type": "text", "analyzer": "korean_analyzer"},
                        
                        # [Vector] í˜œíƒë³„ ì„ë² ë”©
                        "vector": {
                            "type": "dense_vector",
                            "dims": EMBEDDING_DIM,
                            "index": True,
                            "similarity": "cosine"
                        },
                        
                        # [Struct] íŒŒì‹±ëœ ì¡°ê±´(Tier) ì •ë³´
                        "tiers": {
                            "type": "nested",
                            "properties": {
                                "min_spend": {"type": "long"},
                                "max_cap": {"type": "long"},
                                "rate": {"type": "float"},
                                "unit": {"type": "keyword"}
                            }
                        }
                    }
                }
            }
        }
    }

    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {INDEX_NAME}")
    else:
        print(f"â„¹ï¸ ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {INDEX_NAME}")

# ============================================================
# 3. í…ìŠ¤íŠ¸ íŒŒì‹± ë¡œì§ (Parsing Logic)
# ============================================================
def extract_tiers_from_text(text: str, default_value: float) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ 'ì „ì›”ì‹¤ì 'ê³¼ 'í•œë„/ì ë¦½ë¥ ' êµ¬ì¡° ì¶”ì¶œ
    """
    tiers = []
    
    # ì˜ˆ: "40ë§Œì› ì´ìƒ... 1ë§Œ í¬ì¸íŠ¸ ì ë¦½" íŒ¨í„´ ë§¤ì¹­
    pattern_tier = re.compile(r'(\d+)ë§Œì›.*?ì´ìƒ.*?(\d+)(ë§Œ|ì²œ)?\s*(ì›|ì |í¬ì¸íŠ¸)')
    
    matches = pattern_tier.findall(text)
    for match in matches:
        min_spend_str, cap_str, unit_big, unit_type = match
        
        # ê¸ˆì•¡ ê³„ì‚°
        min_spend = int(min_spend_str) * 10000
        
        cap = int(cap_str)
        if unit_big == 'ë§Œ': cap *= 10000
        elif unit_big == 'ì²œ': cap *= 1000
        
        tiers.append({
            "min_spend": min_spend,
            "max_cap": cap,
            "rate": default_value / 100.0 if default_value else 0.0,
            "unit": "KRW" if unit_type == "ì›" else "POINT"
        })
    
    # ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not tiers:
        tiers.append({
            "min_spend": 0,
            "max_cap": -1,  # í•œë„ ì—†ìŒ ì‹ë³„ìš©
            "rate": default_value / 100.0 if default_value else 0.0,
            "unit": "UNKNOWN"
        })
        
    return tiers

# ============================================================
# 4. ë°ì´í„° ë³€í™˜ ë° ì„ë² ë”© ì²˜ë¦¬ (Worker)
# ============================================================
def process_single_card(card_id: str, chunks: List[Dict], embedding_model) -> Dict:
    """
    [Worker í•¨ìˆ˜]
    í•˜ë‚˜ì˜ ì¹´ë“œ IDì— ì†í•œ ì—¬ëŸ¬ ì²­í¬ë¥¼ ëª¨ì•„ì„œ
    1. í…ìŠ¤íŠ¸ íŒŒì‹± (Tiers)
    2. ì„ë² ë”© ìƒì„± (LangChain ì´ìš©)
    3. Nested ë¬¸ì„œ êµ¬ì¡°ë¡œ ë³€í™˜
    """
    if not chunks:
        return None

    try:
        # 1. ì¹´ë“œ ê¸°ë³¸ ì •ë³´ (ì²« ë²ˆì§¸ ì²­í¬ ë©”íƒ€ë°ì´í„° í™œìš©)
        base_meta = chunks[0]["metadata"]
        card_doc = {
            "_id": card_id,
            "_index": INDEX_NAME,
            "_source": {
                "card_id": card_id,
                "card_name": base_meta.get("card_name", "Unknown"),
                "card_company": base_meta.get("card_company", ""),
                "annual_fee": base_meta.get("domestic_year_cost", 0),
                "benefits": []  # ì—¬ê¸°ì— í˜œíƒë“¤ì´ ìŒ“ì„
            }
        }

        # 2. ê° í˜œíƒ(Chunk) ì²˜ë¦¬
        for chunk in chunks:
            meta = chunk["metadata"]
            content = chunk["page_content"]
            
            # (A) í…ìŠ¤íŠ¸ íŒŒì‹± (Tiers êµ¬ì¡°í™”)
            default_val = meta.get("benefit_value", 0)
            tiers = extract_tiers_from_text(content, default_val)
            
            # (B) ì„ë² ë”© ìƒì„± (LangChain ì‚¬ìš©)
            # ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ 'ìš”ì•½ + ìƒì„¸ë‚´ìš©'ì„ í•©ì³ì„œ ë²¡í„°í™”
            text_to_embed = f"{meta.get('benefit_summary', '')} {content}"
            
            # LangChainì˜ embed_query ì‚¬ìš© (ë™ê¸° í˜¸ì¶œ)
            vector = embedding_model.embed_query(text_to_embed)
            
            # (C) í˜œíƒ ê°ì²´ ì¡°ë¦½
            benefit_obj = {
                "category": meta.get("category", "ê¸°íƒ€"),
                "summary": meta.get("benefit_summary", ""),
                "description": content,
                "vector": vector,  # Dense Vector
                "tiers": tiers     # Nested Structure
            }
            
            card_doc["_source"]["benefits"].append(benefit_obj)
            
        return card_doc

    except Exception as e:
        print(f"âš ï¸ ì¹´ë“œ {card_id} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        return None

# ============================================================
# 5. ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ============================================================
def main():
    if not HF_API_KEY:
        raise ValueError("HF_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # 1. Elasticsearch ì—°ê²°
    es = Elasticsearch(ELASTICSEARCH_URL)
    if not es.ping():
        raise ConnectionError("Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    create_index_with_mapping(es)

    # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (LangChain)
    print(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBEDDING_MODEL_ID}")
    embeddings = HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_MODEL_ID,
        task="feature-extraction",
        huggingfacehub_api_token=HF_API_KEY,
    )

    # 3. ë°ì´í„° ë¡œë“œ ë° ê·¸ë£¹í™”
    # JSON íŒŒì¼ì€ í˜œíƒ(ì²­í¬) ë‹¨ìœ„ë¡œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ, ì¹´ë“œ IDë¡œ ë¬¶ì–´ì•¼ í•¨
    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"{INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    card_groups = defaultdict(list)
    for item in raw_data:
        cid = item["metadata"].get("card_id")
        if cid:
            card_groups[cid].append(item)

    # 4. ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    processed_ids = set()
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            processed_ids = set(json.load(f))
            
    # ì²˜ë¦¬í•  ëŒ€ìƒ í•„í„°ë§
    target_card_ids = [cid for cid in card_groups.keys() if cid not in processed_ids]
    target_groups = [(cid, card_groups[cid]) for cid in target_card_ids]
    
    # ==========================================
    # ğŸ›‘ [ìˆ˜ì •] í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10ê°œë§Œ ìë¥´ê¸°
    # ==========================================
    target_groups = target_groups[:10]
    # ==========================================

    print(f"ğŸ“Š ì „ì²´ ì¹´ë“œ: {len(card_groups)}ê°œ | ì™„ë£Œ: {len(processed_ids)}ê°œ | ì‘ì—… ì˜ˆì •: {len(target_groups)}ê°œ")

    if not target_groups:
        print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 5. ë°°ì¹˜ ì²˜ë¦¬ ë° ë³‘ë ¬ ì‹¤í–‰
    # target_groups ë¦¬ìŠ¤íŠ¸ë¥¼ BATCH_SIZEë§Œí¼ ìª¼ê°œì„œ ì²˜ë¦¬
    batches = [target_groups[i:i + BATCH_SIZE] for i in range(0, len(target_groups), BATCH_SIZE)]
    
    total_indexed = 0
    
    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (Workers: {MAX_WORKERS}, Batch Size: {BATCH_SIZE})")
    
    # ThreadPoolExecutor ì‹œì‘
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for batch in tqdm(batches, desc="Total Progress"):
            # ë°°ì¹˜ ë‚´ ê° ì¹´ë“œë¥¼ ë³‘ë ¬ë¡œ ë³€í™˜/ì„ë² ë”©
            futures = []
            for cid, chunks in batch:
                # embeddings ê°ì²´ë¥¼ ì¸ìë¡œ ì „ë‹¬ (Thread-safe ê°€ì •)
                futures.append(executor.submit(process_single_card, cid, chunks, embeddings))
            
            bulk_docs = []
            completed_ids = []
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                result_doc = future.result()
                if result_doc:
                    bulk_docs.append(result_doc)
                    completed_ids.append(result_doc["_source"]["card_id"])
            
            # Elasticsearch Bulk ì ì¬
            if bulk_docs:
                try:
                    success, failed = helpers.bulk(es, bulk_docs, stats_only=True)
                    if failed:
                        print(f"\nâš ï¸ {failed}ê±´ ì¸ë±ì‹± ì‹¤íŒ¨")
                    
                    # ì„±ê³µì ìœ¼ë¡œ ì ì¬ëœ IDë§Œ ì²´í¬í¬ì¸íŠ¸ì— ê¸°ë¡
                    processed_ids.update(completed_ids)
                    with open(CHECKPOINT_FILE, 'w') as f:
                        json.dump(list(processed_ids), f)
                        
                    total_indexed += len(completed_ids)

                except Exception as e:
                    print(f"\nâŒ ES Bulk Error: {e}")
            
            # API Rate Limit ì¡°ì ˆìš© ë”œë ˆì´ (ì„ íƒì‚¬í•­)
            # time.sleep(0.5)

    print(f"\nğŸ‰ ì‘ì—… ì¢…ë£Œ! ì´ {total_indexed}ê°œì˜ ì¹´ë“œê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()