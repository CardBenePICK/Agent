import os
import json
import time
# import re  <-- ì •ê·œì‹ ëª¨ë“ˆ ë¶ˆí•„ìš”
import threading
from typing import List, Dict, Any
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv
from tqdm import tqdm

from langchain_huggingface import HuggingFaceEndpointEmbeddings
from elasticsearch import Elasticsearch, helpers

load_dotenv()

# ============================================================
# 1. ì„¤ì • (Configuration)
# ============================================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "credit_cards_nested_v1"  # êµ¬ì¡°ê°€ ë°”ê¼ˆìœ¼ë‹ˆ ì¸ë±ìŠ¤ ì´ë¦„ ë³€ê²½ ê¶Œì¥
INPUT_FILE = "processed_card_chunks_only_credit_1129.json"
CHECKPOINT_FILE = "ingest_nested_checkpoint_simple.json"

EMBEDDING_MODEL_ID = "BAAI/bge-m3"
EMBEDDING_DIM = 1024

BATCH_SIZE = 8
MAX_WORKERS = 4

# ============================================================
# 2. Elasticsearch ë§¤í•‘ (Schema)
# ============================================================
def create_index_with_mapping(es: Elasticsearch):
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "korean_analyzer": {
                        "type": "nori",
                        "tokenizer": "nori_tokenizer"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                # --- Root Level ---
                "card_id": {"type": "keyword"},
                "card_name": {"type": "text", "analyzer": "korean_analyzer"},
                "card_company": {"type": "keyword"},
                "domestic_year_cost": {"type": "integer"},
                "abroad_year_cost": {"type": "integer"},
                "previous_month_performance": {"type": "integer"},
                
                # --- Nested Level ---
                "benefits": {
                    "type": "nested",
                    "properties": {
                        "category": {"type": "keyword"},
                        "summary": {"type": "text", "analyzer": "korean_analyzer"},
                        "description": {"type": "text", "analyzer": "korean_analyzer"},
                        
                        "vector": {
                            "type": "dense_vector",
                            "dims": EMBEDDING_DIM,
                            "index": True,
                            "similarity": "cosine"
                        },
                        
                        # [ìˆ˜ì •] ë‹¨ìˆœí™”ëœ Tiers êµ¬ì¡°
                        "tiers": {
                            "type": "nested",
                            "properties": {
                                "previous_min_spend": {"type": "long"},     # ì¹´ë“œì˜ ì „ì›”ì‹¤ì 
                                "rate": {"type": "float"},         # benefit_value
                                "unit": {"type": "keyword"},       # benefit_unit (%)
                                "type": {"type": "keyword"}        # benefit_type (saving/discount ë“±) [ì¶”ê°€ë¨]
                            }
                        }
                    }
                }
            }
        }
    }

    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {INDEX_NAME}")
    else:
        print(f"â„¹ï¸ ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {INDEX_NAME}")

# ============================================================
# 3. ë°ì´í„° êµ¬ì¡°í™” ë¡œì§ (Logic Changed)
# ============================================================
def create_tier_from_metadata(meta: Dict, card_min_spend: int) -> List[Dict]:
    """
    [ìˆ˜ì •ë¨] ë³µì¡í•œ í…ìŠ¤íŠ¸ íŒŒì‹± ì—†ì´ ë©”íƒ€ë°ì´í„° ê°’ì„ ê·¸ëŒ€ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.
    """
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    val = meta.get("benefit_value", 0)
    unit = meta.get("benefit_unit", "UNKNOWN")
    b_type = meta.get("benefit_type", "UNKNOWN")
    
    # --- [ì•ˆì „ì¥ì¹˜ ì¶”ê°€] ---
    try:
        rate = float(val)
    except (ValueError, TypeError):
        rate = 0.0
    # ----------------------

    # rate ê³„ì‚° (% ë‹¨ìœ„ ì²˜ë¦¬)
    if unit == "%" and rate > 0:
        rate = rate / 100.0

    return [{
        "previous_min_spend": card_min_spend,
        "rate": rate,
        "unit": unit,
        "type": b_type
    }]


# ============================================================
# 4. ë°ì´í„° ë³€í™˜ ë° ì„ë² ë”© ì²˜ë¦¬ (Worker)
# ============================================================
def process_single_card(card_id: str, chunks: List[Dict], embedding_model) -> Dict:
    if not chunks: return None

    try:
        base_meta = chunks[0]["metadata"]
        
        # Root í•„ë“œ ê°’
        domestic_cost = int(base_meta.get("domestic_year_cost", 0))
        abroad_cost = int(base_meta.get("abroad_year_cost", 0))
        prev_perf = int(base_meta.get("previous_month_performance", 0))
        
        card_doc = {
            "_id": card_id,
            "_index": INDEX_NAME,
            "_source": {
                "card_id": card_id,
                "card_name": base_meta.get("card_name", "Unknown"),
                "card_company": base_meta.get("card_company", ""),
                "domestic_year_cost": domestic_cost,
                "abroad_year_cost": abroad_cost,
                "previous_month_performance": prev_perf,
                "benefits": []
            }
        }

        for chunk in chunks:
            meta = chunk["metadata"]
            content = chunk["page_content"]
            
            # (A) [ë³€ê²½] íŒŒì‹± ëŒ€ì‹  ë©”íƒ€ë°ì´í„° ë§¤í•‘ í•¨ìˆ˜ í˜¸ì¶œ
            # ì¸ìë¡œ ì¹´ë“œì˜ ì „ì›”ì‹¤ì (prev_perf)ì„ ë„˜ê²¨ì„œ previous_min_spendë¡œ ì‚¬ìš©
            tiers = create_tier_from_metadata(meta, prev_perf)
            
            # (B) ì„ë² ë”© ìƒì„±
            text_to_embed = f"{meta.get('benefit_summary', '')} {content}"
            vector = embedding_model.embed_query(text_to_embed)
            
            # (C) í˜œíƒ ê°ì²´ ì¡°ë¦½
            benefit_obj = {
                "category": meta.get("category", "ê¸°íƒ€"),
                "summary": meta.get("benefit_summary", ""),
                "description": content,
                "vector": vector,
                "tiers": tiers
            }
            
            card_doc["_source"]["benefits"].append(benefit_obj)
            
        return card_doc

    except Exception as e:
        print(f"âš ï¸ ì¹´ë“œ {card_id} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        return None

# ============================================================
# 5. ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ============================================================
def main():
    if not HF_API_KEY:
        raise ValueError("HF_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    es = Elasticsearch(ELASTICSEARCH_URL)
    if not es.ping():
        raise ConnectionError("Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    create_index_with_mapping(es)

    print(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBEDDING_MODEL_ID}")
    embeddings = HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_MODEL_ID,
        task="feature-extraction",
        huggingfacehub_api_token=HF_API_KEY,
    )

    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"{INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    card_groups = defaultdict(list)
    for item in raw_data:
        cid = item["metadata"].get("card_id")
        if cid:
            card_groups[cid].append(item)

    processed_ids = set()
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            processed_ids = set(json.load(f))
            
    target_card_ids = [cid for cid in card_groups.keys() if cid not in processed_ids]
    target_groups = [(cid, card_groups[cid]) for cid in target_card_ids]
    
    # í…ŒìŠ¤íŠ¸ìš© 10ê°œ ì œí•œ (í•„ìš”ì‹œ ì£¼ì„ ì²˜ë¦¬)
    # target_groups = target_groups[:10]

    print(f"ğŸ“Š ì „ì²´ ì¹´ë“œ: {len(card_groups)}ê°œ | ì™„ë£Œ: {len(processed_ids)}ê°œ | ì‘ì—… ì˜ˆì •: {len(target_groups)}ê°œ")

    if not target_groups:
        print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    batches = [target_groups[i:i + BATCH_SIZE] for i in range(0, len(target_groups), BATCH_SIZE)]
    total_indexed = 0
    
    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (Workers: {MAX_WORKERS}, Batch Size: {BATCH_SIZE})")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for batch in tqdm(batches, desc="Total Progress"):
            futures = []
            for cid, chunks in batch:
                futures.append(executor.submit(process_single_card, cid, chunks, embeddings))
            
            bulk_docs = []
            completed_ids = []
            
            for future in as_completed(futures):
                result_doc = future.result()
                if result_doc:
                    bulk_docs.append(result_doc)
                    completed_ids.append(result_doc["_source"]["card_id"])
            
            if bulk_docs:
                try:
                    success, failed = helpers.bulk(es, bulk_docs, stats_only=True)
                    if failed:
                        print(f"\nâš ï¸ {failed}ê±´ ì¸ë±ì‹± ì‹¤íŒ¨")
                    
                    processed_ids.update(completed_ids)
                    with open(CHECKPOINT_FILE, 'w') as f:
                        json.dump(list(processed_ids), f)
                        
                    total_indexed += len(completed_ids)

                except Exception as e:
                    print(f"\nâŒ ES Bulk Error: {e}")

    print(f"\nğŸ‰ ì‘ì—… ì¢…ë£Œ! ì´ {total_indexed}ê°œì˜ ì¹´ë“œê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()