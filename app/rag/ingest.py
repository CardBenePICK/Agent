import os
import json
import time  # âœ… í•„ìˆ˜ ì¶”ê°€!
from typing import List
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from dotenv import load_dotenv

load_dotenv()

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "card_benefit_bgem3_v1"

def load_processed_docs(json_path: str) -> List[Document]:
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        documents = []
        for item in data:
            doc = Document(
                page_content=item["page_content"],
                metadata=item["metadata"]
            )
            documents.append(doc)
        print(f"âœ… JSONì—ì„œ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def ingest_documents(docs: List[Document]):
    try:
        if not HF_API_KEY:
            raise ValueError("HF_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print(f"ğŸš€ Hugging Face API ì—°ê²° ì¤‘... (Model: BAAI/bge-m3)")
        
        embeddings = HuggingFaceEndpointEmbeddings(
            model="BAAI/bge-m3",
            task="feature-extraction",
            huggingfacehub_api_token=HF_API_KEY,
        )
        
        print(f"ğŸš€ Elasticsearch({ELASTICSEARCH_URL})ì— ì ì¬ ì‹œì‘...")
        
        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL,
            index_name=INDEX_NAME,
            embedding=embeddings
        )
        
        # âœ… ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¶•ì†Œ (200 -> 32)
        # API íƒ€ì„ì•„ì›ƒ(504) ë°©ì§€ë¥¼ ìœ„í•´ ì•„ì£¼ ì‘ê²Œ ìª¼ê°­ë‹ˆë‹¤.
        batch_size = 32  
        total_docs = len(docs)
        
        for i in range(0, total_docs, batch_size):
            batch = docs[i : i + batch_size]
            print(f"ğŸ“¦ API ì „ì†¡ ì¤‘... ({i + 1}/{total_docs}) - {len(batch)}ê°œ")
            
            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 5íšŒë¡œ ì¦ê°€)
            max_retries = 5
            for attempt in range(max_retries):
                try:
                    vector_store.add_documents(batch)
                    break # ì„±ê³µí•˜ë©´ íƒˆì¶œ
                except Exception as e:
                    print(f"âš ï¸ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                    # ëŒ€ê¸° ì‹œê°„ ì ì§„ì  ì¦ê°€ (5ì´ˆ, 10ì´ˆ, 15ì´ˆ...)
                    wait_time = (attempt + 1) * 5
                    print(f"â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    time.sleep(wait_time)
            else:
                # break ì—†ì´ ë°˜ë³µë¬¸ì´ ëë‚œ ê²½ìš° (ëª¨ë“  ì‹œë„ ì‹¤íŒ¨)
                print(f"âŒ ë°°ì¹˜ {i} ì ì¬ ìµœì¢… ì‹¤íŒ¨. ê±´ë„ˆëœë‹ˆë‹¤.")

        print(f"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        return vector_store
        
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    JSON_FILE_PATH = os.path.join(current_dir, "processed_card_chunks.json")
    
    if os.path.exists(JSON_FILE_PATH):
        docs = load_processed_docs(JSON_FILE_PATH)
        ingest_documents(docs)
    else:
        print(f"âš ï¸ '{JSON_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")