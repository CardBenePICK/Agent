import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def load_documents(file_path: str):
    """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        print(f"âœ… Successfully loaded {len(documents)} documents from {file_path}")
        return documents
    except Exception as e:
        print(f"âŒ Error loading documents: {e}")
        raise

def split_documents(documents):
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    try:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50
        )
        docs = text_splitter.split_documents(documents)
        print(f"âœ… Successfully split documents into {len(docs)} chunks")
        return docs
    except Exception as e:
        print(f"âŒ Error splitting documents: {e}")
        raise

def ingest_documents(docs, index_name: str):
    """ë¬¸ì„œë¥¼ Elasticsearchì— ì¸ë±ì‹±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # API í‚¤ í™•ì¸
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        print("ğŸ”‘ Creating OpenAI embeddings...")
        # OpenAI ì„ë² ë”© ìƒì„± - í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ë¥¼ ì½ìŒ
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"  # ìµœì‹  ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        )
        
        print("ğŸ“Š Testing embeddings...")
        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        test_embed = embeddings.embed_query("test")
        print(f"âœ… Embeddings working! Dimension: {len(test_embed)}")

        print("ğŸ” Connecting to Elasticsearch...")
        db = ElasticsearchStore.from_documents(
            docs,
            embeddings,
            es_url=ELASTICSEARCH_URL,
            index_name=index_name
        )
        
        print(f"âœ… Documents indexed to Elasticsearch index '{index_name}' successfully.")
        return db
        
    except Exception as e:
        print(f"âŒ Error ingesting documents: {e}")
        raise

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„±
        sample_text = """
LangChain is an open-source framework designed to help developers build applications with Large Language Models (LLMs).
It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.

LangGraph is a library for building stateful, multi-actor applications with LLMs, modeling steps as nodes and edges in a graph.
It extends the LangChain Expression Language with the ability to coordinate multiple chains across multiple steps of computation.

FastAPI is a modern, high-performance web framework for building APIs with Python.
It is based on standard Python type hints and provides automatic API documentation.

Elasticsearch is a distributed search and analytics engine, which can also serve as a powerful vector store for RAG.
It provides scalable full-text search and supports vector similarity search for semantic retrieval.

LangSmith is a platform for debugging, testing, and monitoring AI applications.
It helps developers trace, evaluate, and monitor their LangChain applications in production.
"""

        print("ğŸ“ Creating sample document...")
        with open("example_docs.txt", "w", encoding='utf-8') as f:
            f.write(sample_text)

        print("ğŸ“š Loading documents...")
        documents = load_documents("example_docs.txt")
        
        print("âœ‚ï¸  Splitting documents...")
        chunks = split_documents(documents)
        
        print("ğŸš€ Ingesting documents to Elasticsearch...")
        ingest_documents(chunks, "llm_rag_index")
        
        print("ğŸ‰ All done! Documents have been successfully indexed.")
        
        print(ELASTICSEARCH_URL)
    except Exception as e:
        print(f"ğŸ’¥ Error in main process: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()