import os
import json
from typing import List
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
INDEX_NAME = "card_benefit2080_index_v1"  # ì¸ë±ìŠ¤ ì´ë¦„ ë³€ê²½ ê¶Œì¥

def load_processed_docs(json_path: str) -> List[Document]:
    """ì „ì²˜ë¦¬ëœ JSON íŒŒì¼ì„ ì½ì–´ LangChain Document ê°ì²´ë¡œ ë³€í™˜"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        documents = []
        for item in data:
            # page_contentì™€ metadataê°€ í™•ì‹¤íˆ ë¶„ë¦¬ë˜ì–´ ìˆì–´ì•¼ í•¨
            doc = Document(
                page_content=item["page_content"],
                metadata=item["metadata"]
            )
            documents.append(doc)
            
        print(f"âœ… JSONì—ì„œ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def ingest_documents(docs: List[Document]):
    """Elasticsearchì— ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì ì¬ (ë°°ì¹˜ ì²˜ë¦¬ ì ìš©)"""
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        print(f"ğŸš€ Elasticsearch({ELASTICSEARCH_URL})ì— ì ì¬ ì‹œì‘...")
        
        # 1. Vector Store ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (ë°ì´í„° ì—†ì´ ì—°ê²°ë§Œ ì„¤ì •)
        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL,
            index_name=INDEX_NAME,
            embedding=embeddings
        )
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • (í•œ ë²ˆì— 1000ê°œì”©)
        batch_size = 200
        total_docs = len(docs)
        
        # 3. ë°˜ë³µë¬¸ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì ì¬
        for i in range(0, total_docs, batch_size):
            batch = docs[i : i + batch_size]
            print(f"ğŸ“¦ ë°°ì¹˜ ì ì¬ ì¤‘... ({i + 1}/{total_docs}) - {len(batch)}ê°œ ë¬¸ì„œ")
            
            # add_documents í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¶”ê°€
            vector_store.add_documents(batch)
            
        print(f"ğŸ‰ ëª¨ë“  ì ì¬ ì™„ë£Œ! ì´ {total_docs}ê°œ ë¬¸ì„œê°€ '{INDEX_NAME}' ì¸ë±ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return vector_store
        
    except Exception as e:
        print(f"âŒ ì ì¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    # 1. í˜„ì¬ ì´ íŒŒì´ì¬ íŒŒì¼(ingest.py)ì´ ìˆëŠ” í´ë” ê²½ë¡œë¥¼ êµ¬í•¨
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # 2. ê·¸ í´ë” ì•ˆì— ìˆëŠ” processed_card_chunks.json íŒŒì¼ì„ ì§€ì •
    JSON_FILE_PATH = os.path.join(current_dir, "processed_card_chunks.json")
    
    print(f"ğŸ“‚ íŒŒì¼ ì°¾ëŠ” ê²½ë¡œ: {JSON_FILE_PATH}")  # ê²½ë¡œ í™•ì¸ìš© ì¶œë ¥

    if os.path.exists(JSON_FILE_PATH):
        docs = load_processed_docs(JSON_FILE_PATH)
        ingest_documents(docs)
    else:
        print(f"âš ï¸ '{JSON_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì„œ íŒŒì¼ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")