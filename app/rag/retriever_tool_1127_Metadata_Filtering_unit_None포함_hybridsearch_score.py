import os
import sys
import requests
import json
import time
import re
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from collections import defaultdict

# LangChain Imports
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain_elasticsearch import ElasticsearchStore
from langchain_core.tools import tool
from langsmith import traceable
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY") or os.getenv("HF_TOKEN")
INDEX_NAME = "card_benefit_bgem3_v2"

MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct:novita" 
EMBEDDING_MODEL_ID = "intfloat/multilingual-e5-large" # API ì•ˆì •ì„± ìœ„í•´ e5 ì‚¬ìš©

# ============================================================
# 0. ì¹´í…Œê³ ë¦¬ ì‚¬ì „ ë¡œë“œ
# ============================================================

try:
    from category_dictionary import KNOWN_CATEGORIES
    print(f"âœ… ì¹´í…Œê³ ë¦¬ ì‚¬ì „ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ({len(KNOWN_CATEGORIES)}ê°œ í•­ëª©)")
except ImportError:
    print("âš ï¸ 'category_dictionary.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    KNOWN_CATEGORIES = []

# ============================================================
# 1. ìœ í‹¸ë¦¬í‹°
# ============================================================

def safe_print(text):
    try: print(text)
    except: pass

def sanitize_text(text: str) -> str:
    if not isinstance(text, str): return str(text)
    try: return text.encode('utf-8', 'ignore').decode('utf-8')
    except: return ""

def query_hf_chat_api(payload):
    chat_url = "https://router.huggingface.co/v1/chat/completions"
    headers = {"Authorization": f"Bearer {HF_API_KEY}", "Content-Type": "application/json"}
    try:
        response = requests.post(chat_url, headers=headers, json=payload)
        return response.json()
    except Exception as e:
        print(f"âŒ Chat API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

# ============================================================
# 2. Custom Embedding Class
# ============================================================

class CustomHFEmbeddings(Embeddings):
    def __init__(self, api_key, model_id):
        self.api_key = api_key
        self.api_url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
        self.headers = {"Authorization": f"Bearer {api_key}"}
        self.model_id = model_id

    def _query(self, texts: List[str]) -> Any:
        clean_texts = [sanitize_text(t) for t in texts]
        try:
            response = requests.post(
                self.api_url, 
                headers=self.headers, 
                json={"inputs": clean_texts, "options": {"wait_for_model": True}}
            )
            return response.json()
        except Exception as e:
            print(f"âŒ [API í†µì‹  ì—ëŸ¬] {e}")
            return []

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = self._query(texts)
        if isinstance(result, list): return result
        return []

    @lru_cache(maxsize=1000)
    def embed_query(self, text: str) -> List[float]:
        if "e5" in self.model_id: text = f"query: {sanitize_text(text)}"
        else: text = sanitize_text(text)
            
        result = self._query([text])
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list): return result[0]
            elif isinstance(result[0], float): return result
        return []

# ============================================================
# 3. í†µí•© ë¶„ì„ í•¨ìˆ˜ (LLM)
# ============================================================

@traceable(name="0. Analyze Query Unified", run_type="llm")
def analyze_query_unified(original_query: str) -> Dict:
    found_categories = []
    query_lower = original_query.lower()
    for cat in KNOWN_CATEGORIES:
        if cat.lower() in query_lower:
            found_categories.append(cat)
    
    if not HF_API_KEY:
        return {"brands": [], "categories": list(set(found_categories)), "expanded_queries": [original_query]}

    categories_str = ", ".join(KNOWN_CATEGORIES)
    messages = [
        {
            "role": "system",
            "content": "You are an expert in search query analysis. Extract information from the user's query and output it in JSON format."
        },
        {
            "role": "user",
            "content": f"""
ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ 3ê°€ì§€ ì •ë³´ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜.

1. **brands**: ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ë¸Œëœë“œë‚˜ ì„œë¹„ìŠ¤ëª… (ì˜ˆ: "ìŠ¤ë²…"->"ìŠ¤íƒ€ë²…ìŠ¤", "í†¡í†¡O"). ì¼ë°˜ ëª…ì‚¬ ì œì™¸.
2. **categories**: ì•„ë˜ [ì¹´í…Œê³ ë¦¬ ëª©ë¡] ì¤‘ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ê²ƒë“¤.
3. **expanded_queries**: ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í™•ì¥ ê²€ìƒ‰ì–´ **ìµœëŒ€ 3ê°œ**.

[ì¹´í…Œê³ ë¦¬ ëª©ë¡]
{categories_str}

[ì‚¬ìš©ì ì§ˆë¬¸]
"{original_query}"

[ì¶œë ¥ ì˜ˆì‹œ]
{{
  "brands": ["ìŠ¤íƒ€ë²…ìŠ¤"],
  "categories": ["ì¹´í˜"],
  "expanded_queries": ["ìŠ¤íƒ€ë²…ìŠ¤ í• ì¸ ì¹´ë“œ", "ì¹´í˜ í˜œíƒ"]
}}

ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´.
"""
        }
    ]

    payload = {"model": MODEL_NAME, "messages": messages, "max_tokens": 500, "temperature": 0.1, "response_format": {"type": "json_object"}}
    response_data = query_hf_chat_api(payload)
    
    result = {"brands": [], "categories": list(set(found_categories)), "expanded_queries": [original_query]}

    if response_data and "choices" in response_data:
        try:
            content = response_data["choices"][0]["message"]["content"]
            clean_text = content.replace("```json", "").replace("```", "").strip()
            clean_text = sanitize_text(clean_text)
            parsed = json.loads(clean_text)
            
            if "brands" in parsed: result["brands"] = parsed["brands"]
            if "categories" in parsed: 
                valid_cats = [c for c in parsed["categories"] if c in KNOWN_CATEGORIES]
                result["categories"].extend(valid_cats)
            if "expanded_queries" in parsed:
                cleaned_queries = [sanitize_text(q) for q in parsed["expanded_queries"]]
                cleaned_queries.insert(0, sanitize_text(original_query))
                result["expanded_queries"] = cleaned_queries
        except: pass

    result["brands"] = list(set(result["brands"]))
    result["categories"] = list(set(result["categories"]))
    result["expanded_queries"] = list(dict.fromkeys(result["expanded_queries"]))

    return result

# ============================================================
# 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„ (Vector + Keyword + RRF)
# ============================================================

def perform_vector_search(vector_store, query: str, k: int, categories: List[str]) -> List[Any]:
    query_vector = vector_store.embedding.embed_query(query)
    if not query_vector: 
        print(f"âš ï¸ [Vector Search Skip] ì„ë² ë”© ì‹¤íŒ¨")
        return []
    
    es_filter = []
    if categories:
        es_filter.append({"terms": {"metadata.category.keyword": categories}})

    knn_query = {
        "field": "vector", 
        "query_vector": query_vector,
        "k": k,
        "num_candidates": max(100, k * 2),
        "filter": es_filter
    }
    
    try:
        # size íŒŒë¼ë¯¸í„° ì¶”ê°€!
        response = vector_store.client.search(index=INDEX_NAME, body={
            "knn": knn_query, "size": k, "_source": ["text", "metadata", "page_content"]
        })
        results = []
        for hit in response["hits"]["hits"]:
            score = hit["_score"]
            source = hit["_source"]
            content = source.get("text") or source.get("page_content") or ""
            doc = Document(page_content=content, metadata=source.get("metadata", {}))
            results.append((doc, score))
        
        print(f"âœ… [Vector] '{query}' -> {len(results)}ê±´")
        return results
    except Exception as e:
        print(f"âŒ [Vector Error] {e}")
        return []

def perform_keyword_search(vector_store, query: str, k: int, categories: List[str]) -> List[Any]:
    es_filter = []
    if categories:
        es_filter.append({"terms": {"metadata.category.keyword": categories}})

    query_nospace = query.replace(" ", "")
    
    match_query = {
        "bool": {
            "should": [
                {"match": {"text": query}},          
                {"match": {"text": query_nospace}}   
            ],
            "minimum_should_match": 1,
            "filter": es_filter
        }
    }

    try:
        response = vector_store.client.search(index=INDEX_NAME, body={
            "query": match_query, "size": k, "_source": ["text", "metadata", "page_content"]
        })
        results = []
        for hit in response["hits"]["hits"]:
            score = hit["_score"]
            source = hit["_source"]
            content = source.get("text") or source.get("page_content") or ""
            doc = Document(page_content=content, metadata=source.get("metadata", {}))
            results.append((doc, score))
            
        print(f"âœ… [Keyword] '{query}' -> {len(results)}ê±´")
        return results
    except: return []

def apply_rrf(vector_results, keyword_results, k=60):
    fusion_scores = defaultdict(float)
    doc_map = {}
    c = 60

    for rank, (doc, score) in enumerate(vector_results):
        doc_id = doc.page_content
        if doc_id not in doc_map:
            doc.metadata["vec_score"] = score 
            doc.metadata["key_score"] = 0.0   
            doc_map[doc_id] = doc
        else:
            doc_map[doc_id].metadata["vec_score"] = score
        fusion_scores[doc_id] += 1 / (rank + c)

    for rank, (doc, score) in enumerate(keyword_results):
        doc_id = doc.page_content
        if doc_id not in doc_map:
            doc.metadata["key_score"] = score 
            doc.metadata["vec_score"] = 0.0   
            doc_map[doc_id] = doc
        else:
            doc_map[doc_id].metadata["key_score"] = score
        fusion_scores[doc_id] += 1 / (rank + c)

    sorted_docs = sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)
    
    final_results = []
    for doc_id, rrf_score in sorted_docs[:k]:
        doc = doc_map[doc_id]
        doc.metadata["rrf_score"] = rrf_score 
        final_results.append(doc) 
    return final_results

@traceable(name="1. Retrieve Candidates (Hybrid+Parallel)", run_type="retriever")
def retrieve_candidates(vector_store, queries: List[str], k_per_query: int, categories: List[str]):
    all_docs = []
    unique_queries = list(set([sanitize_text(q) for q in queries if q.strip()]))

    def _run_vector(q): return perform_vector_search(vector_store, q, k_per_query, categories)
    def _run_keyword(q): return perform_keyword_search(vector_store, q, k_per_query, categories)

    with ThreadPoolExecutor(max_workers=10) as executor:
        vec_futures = {executor.submit(_run_vector, q): q for q in unique_queries}
        key_futures = {executor.submit(_run_keyword, q): q for q in unique_queries}
        
        vec_results = {}
        key_results = {}
        
        for f in as_completed(vec_futures): vec_results[vec_futures[f]] = f.result()
        for f in as_completed(key_futures): key_results[key_futures[f]] = f.result()

    for q in unique_queries:
        v_res = vec_results.get(q, [])
        k_res = key_results.get(q, [])
        hybrid_docs = apply_rrf(v_res, k_res, k=k_per_query)
        all_docs.extend(hybrid_docs)
    
    unique_docs_map = {}
    for doc in all_docs:
        if doc.page_content not in unique_docs_map:
            unique_docs_map[doc.page_content] = doc
        else:
            if doc.metadata.get("rrf_score", 0) > unique_docs_map[doc.page_content].metadata.get("rrf_score", 0):
                unique_docs_map[doc.page_content] = doc
                
    return list(unique_docs_map.values())

# ============================================================
# 5. ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹ (ì±„ì í‘œ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€ ğŸš€)
# ============================================================

def calculate_benefit_score(doc, user_brands, user_categories) -> Tuple[float, str]:
    """
    ì ìˆ˜ì™€ í•¨ê»˜ 'ê³„ì‚° ë‚´ì—­(Breakdown)'ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        base = float(doc.metadata.get("benefit_value", 0))
        unit = doc.metadata.get("benefit_unit", "").strip()
        doc_cat = doc.metadata.get("category", "")
        summary = doc.metadata.get("benefit_summary", "")
        text = (doc.metadata.get("card_name", "") + " " + summary + " " + (doc.page_content or ""))

        score_log = [] # ì±„ì  ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸

        # 1. í…ìŠ¤íŠ¸ ë§ˆì´ë‹
        extracted_score = 0
        if base < 10:
            pct_matches = re.findall(r'(\d+(?:\.\d+)?)\s*%', text)
            if pct_matches: 
                val = max([float(x) for x in pct_matches]) * 100
                if val > extracted_score: extracted_score = val
                
            krw_matches = re.findall(r'(\d+(?:,\d+)*)\s*ì›', text)
            if krw_matches: 
                val = max([float(x.replace(',', '')) for x in krw_matches])
                if val > extracted_score: extracted_score = val
            
            man_matches = re.findall(r'(\d+(?:,\d+)*)\s*ë§Œì›', text)
            if man_matches: 
                val = max([float(x.replace(',', '')) for x in man_matches]) * 10000
                if val > extracted_score: extracted_score = val
            
            if extracted_score > 0:
                score_log.append(f"í…ìŠ¤íŠ¸ì¶”ì¶œ({extracted_score:.0f})")
        
        # 2. ë©”íƒ€ë°ì´í„° ì ìˆ˜
        meta_score = base * 100 if unit == "%" else base
        if meta_score > 0:
            score_log.append(f"ë©”íƒ€ë°ì´í„°({meta_score:.0f})")
        
        # ìµœì¢… ê¸°ë³¸ ì ìˆ˜ ì„ ì •
        score = max(meta_score, extracted_score)
        if score <= 0: 
            score = 1.0
            score_log.append("ê¸°ë³¸(1.0)")

        # 3. ê°€ì¤‘ì¹˜
        if user_brands and any(b in text for b in user_brands): 
            score *= 2.0
            score_log.append("ë¸Œëœë“œ(x2.0)")
            
        if user_categories:
            if any(c in doc_cat or doc_cat in c for c in user_categories): 
                score *= 1.5
                score_log.append("ì¹´í…Œê³ ë¦¬ì¼ì¹˜(x1.5)")
            elif any(c in text for c in user_categories): 
                score *= 1.2
                score_log.append("ì¹´í…Œê³ ë¦¬í¬í•¨(x1.2)")

        # ê³„ì‚° ë‚´ì—­ ë¬¸ìì—´ ìƒì„± (ì˜ˆ: "ë©”íƒ€ë°ì´í„°(10000) + ë¸Œëœë“œ(x2.0)")
        breakdown_str = " + ".join(score_log)
        return score, breakdown_str

    except: return 0.0, "Error"

@traceable(name="2. Rerank Candidates", run_type="parser")
def rerank_candidates(docs, user_brands, user_categories):
    docs_scored = []
    for doc in docs:
        score, breakdown = calculate_benefit_score(doc, user_brands, user_categories)
        # doc ê°ì²´ì— breakdown ì •ë³´ ì‹¬ê¸° (ì„ì‹œ)
        doc.metadata["score_breakdown"] = breakdown
        docs_scored.append((doc, score))
    return sorted(docs_scored, key=lambda x: x[1], reverse=True)

def select_final_results_with_merge(sorted_docs, top_k, user_brands, user_categories):
    merged_results = {} 
    for doc, score in sorted_docs:
        card_name = doc.metadata.get("card_name", "")
        if not card_name: continue

        summary = doc.metadata.get("benefit_summary", "")
        cat = doc.metadata.get("category", "")
        breakdown = doc.metadata.get("score_breakdown", "") # ì±„ì í‘œ ê°€ì ¸ì˜¤ê¸°
        
        # ê²€ìƒ‰ ì ìˆ˜
        vec_score = doc.metadata.get("vec_score", 0.0)
        key_score = doc.metadata.get("key_score", 0.0)
        rrf_score = doc.metadata.get("rrf_score", 0.0)
        
        if card_name not in merged_results:
            merged_results[card_name] = {
                "card_name": card_name,
                "origin_id": doc.metadata.get("origin_id", ""),
                "summaries": [summary],
                "categories": [cat],
                "total_score": score,
                "match_reason": f"ë¸Œëœë“œ:{user_brands}, ì¹´í…Œê³ ë¦¬:{user_categories}",
                "max_vec": vec_score,
                "max_key": key_score,
                "max_rrf": rrf_score,
                "breakdown_log": [breakdown] # ì±„ì í‘œ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
            }
        else:
            if summary not in merged_results[card_name]["summaries"]:
                merged_results[card_name]["summaries"].append(summary)
            if cat not in merged_results[card_name]["categories"]:
                merged_results[card_name]["categories"].append(cat)
            
            merged_results[card_name]["total_score"] += (score * 0.2)
            merged_results[card_name]["breakdown_log"].append(f"ì¶”ê°€ë§¤ì¹­(+{score*0.2:.0f})") # ê°€ì‚°ì  ë¡œê·¸
            
            merged_results[card_name]["max_vec"] = max(merged_results[card_name]["max_vec"], vec_score)
            merged_results[card_name]["max_key"] = max(merged_results[card_name]["max_key"], key_score)
            merged_results[card_name]["max_rrf"] = max(merged_results[card_name]["max_rrf"], rrf_score)

    final_list = []
    for info in merged_results.values():
        combined_summary = " / ".join(info["summaries"])
        combined_cat = ", ".join(list(set(info["categories"])))
        # ì±„ì í‘œë„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        combined_breakdown = " | ".join(list(set(info["breakdown_log"])))
        
        final_list.append({
            "card_name": info["card_name"],
            "score": info["total_score"],
            "benefit_summary": combined_summary,
            "category": combined_cat,
            "match_reason": info["match_reason"],
            "vec_score": info["max_vec"],
            "key_score": info["max_key"],
            "rrf_score": info["max_rrf"],
            "score_breakdown": combined_breakdown # ìµœì¢… ì±„ì í‘œ
        })

    final_list = sorted(final_list, key=lambda x: x["score"], reverse=True)
    return final_list[:top_k]

# ============================================================
# 6. Main Tool
# ============================================================

@tool
def retriever_tool(query: str) -> List[Dict]:
    """Search for credit card benefits using Hybrid Search & Metadata Filtering."""
    try:
        if not HF_API_KEY: return []

        embeddings = CustomHFEmbeddings(api_key=HF_API_KEY, model_id=EMBEDDING_MODEL_ID)
        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL, index_name=INDEX_NAME, embedding=embeddings
        )

        analysis = analyze_query_unified(query)
        user_brands = analysis["brands"]
        user_cats = analysis["categories"]
        queries = analysis["expanded_queries"]
        
        safe_print(f"\nğŸ‘€ [ë¶„ì„] ë¸Œëœë“œ:{user_brands}, ì¹´í…Œê³ ë¦¬:{user_cats}")
        safe_print(f"   [í™•ì¥ê²€ìƒ‰ì–´]: {queries}")

        candidates = retrieve_candidates(vector_store, queries, 60, user_cats)
        safe_print(f"   ğŸ‘‰ í›„ë³´êµ°: {len(candidates)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

        ranked = rerank_candidates(candidates, user_brands, user_cats)
        return select_final_results_with_merge(ranked, 3, user_brands, user_cats)

    except Exception as e:
        safe_print(f"âŒ ì˜¤ë¥˜: {e}")
        return []

# ============================================================
# 7. ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    if langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
        os.environ["LANGCHAIN_PROJECT"] = "CardBenefit RAG Debug"
        safe_print(f"âœ… LangSmith Tracing Enabled")
    else:
        safe_print("âš ï¸ LangSmith API Key not found.")
    safe_print(f"ğŸ” ìµœì¢… ê³ ë„í™” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Score Breakdown Added)")
    safe_print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥ ë˜ëŠ” Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.\n")
    
    while True:
        try:
            q = input("\nğŸ’¬ ì…ë ¥: ").strip()
        except KeyboardInterrupt:
            safe_print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception: continue

        if q.lower() in ["q", "quit", "exit"]: break
        if not q: continue

        start = time.perf_counter()
        results = retriever_tool.invoke(q)
        elapsed = time.perf_counter() - start

        safe_print(f"\nâ±ï¸ Time: {elapsed:.4f}s")

        if results:
            safe_print(f"ğŸ† [ì¶”ì²œ ê²°ê³¼ Top {len(results)}]")
            for i, res in enumerate(results):
                safe_print(f"{i+1}. {res['card_name']} (ì ìˆ˜: {res['score']:.0f})")
                safe_print(f"   - í˜œíƒ: {res['benefit_summary']}")
                # ìƒì„¸ ì±„ì í‘œ ì¶œë ¥ ğŸš€
                safe_print(f"   - ğŸ“ ì±„ì í‘œ: {res['score_breakdown']}")
                safe_print(f"   - [ê²€ìƒ‰] V:{res['vec_score']:.4f} / K:{res['key_score']:.4f} / RRF:{res['rrf_score']:.4f}")
        else:
            safe_print("âš ï¸ ê²°ê³¼ ì—†ìŒ")