import sys
import io
import os
import json
import requests
import time
from typing import List, Dict, Any
from collections import defaultdict
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from langchain_huggingface import HuggingFaceEndpointEmbeddings

# [í•„ìˆ˜] Docker/Linux í™˜ê²½ì—ì„œ í•œê¸€ ì¶œë ¥ ì—ëŸ¬ ë°©ì§€
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# [LangSmith] ì¶”ì ìš© (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
try:
    from langsmith import traceable
except ImportError:
    def traceable(**kwargs):
        def decorator(func): return func
        return decorator

load_dotenv()

# ============================================================
# âš™ï¸ ì„¤ì • (Configuration)
# ============================================================
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY") or os.getenv("HF_TOKEN")
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct:novita"
EMBEDDING_MODEL_ID = "BAAI/bge-m3"
INDEX_NAME = "credit_cards_nested_top100"

# ì„ë² ë”© ê°ì²´ ì„¤ì •
embeddings = HuggingFaceEndpointEmbeddings(
    model=EMBEDDING_MODEL_ID,
    task="feature-extraction",
    huggingfacehub_api_token=HF_API_KEY,
)

def safe_print(title, data):
    print(f"\nğŸ”¹ [{title}]")
    if isinstance(data, list):
        for i, item in enumerate(data[:3]): # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
            print(f"   {i+1}. {item}")
        if len(data) > 3: print(f"   ... (ì´ {len(data)}ê°œ)")
    else:
        print(f"   {data}")

# ============================================================
# 1. LLM Client
# ============================================================
class LLMClient:
    """[ê³µí†µ] LLM API í˜¸ì¶œê¸°"""
    
    @staticmethod
    @traceable(run_type="llm", name="HF_Inference_API")
    def call_api(messages: List[Dict], temperature=0.1) -> Dict:
        headers = {"Authorization": f"Bearer {HF_API_KEY}"}
        payload = {
            "model": MODEL_NAME, "messages": messages, 
            "max_tokens": 1000, "temperature": temperature,
            "response_format": {"type": "json_object"}
        }
        try:
            resp = requests.post("https://router.huggingface.co/v1/chat/completions", json=payload, headers=headers)
            resp.raise_for_status()
            content = resp.json()['choices'][0]['message']['content']
            
            # --- [íŒŒì‹± ë¡œì§] ---
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            content = content.strip()
            
            start_idx = content.find('{')
            end_idx = content.rfind('}')
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx : end_idx + 1]
                return json.loads(json_str)
            else:
                return json.loads(content)

        except Exception as e:
            print(f"âŒ [LLM Error] {e}")
            return {}

# ============================================================
# 2. Query Analysis Logic
# ============================================================
class QueryAnalyzer:
    def rewrite_and_extract(self, query: str) -> List[str]:
        """Step 1: ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í•µì‹¬ ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        system_prompt = (
            "Extract potential brand or category keywords from the query. "
            "Output JSON ONLY: {\"keywords\": [\"k1\", \"k2\"]}. No notes."
        )
        msg = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Query: {query}"}
        ]
        
        resp = LLMClient.call_api(msg)
        extracted = resp.get("keywords", [])
        safe_print("Step 1: Extracted Keywords", extracted)
        return extracted

    def group_and_weight(self, query: str, keywords: List[str]) -> List[Dict]:
        """Step 2: ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·¸ë£¹í•‘ (ë¶„ë¦¬ ê°•í™” ë²„ì „)"""
        system_prompt = (
            "You are a strict JSON generator. "
            "Your Goal: Split keywords into DISTINCT semantic categories. "
            "Do NOT merge different concepts (e.g., Cafe and Gas must be separate). "
            "Output ONLY the JSON object."
        )

        prompt = f"""
        User Query: "{query}"
        Extracted Keywords: {keywords}

        Rules:
        1. **Separate Strictly**: If keywords belong to different industries (e.g., 'Cafe', 'Gas', 'Convenience', 'Shopping'), create SEPARATE groups for each.
        2. Assign 'weight' (0.5 to 1.0).
        3. 'is_must': true if the user implies it's mandatory (ê¼­, í•„ìˆ˜, must).
        4. 'search_query': Create a specific search query for that single category.

        Example Input: "ì¹´í˜ë‘ ì£¼ìœ ì†Œ í•„ìˆ˜ê³  í¸ì˜ì "
        Example Output:
        {{
            "groups": [
                {{"name": "Cafe", "keywords": ["Cafe"], "weight": 1.0, "is_must": true, "search_query": "ì¹´í˜ ìŠ¤íƒ€ë²…ìŠ¤ í• ì¸"}},
                {{"name": "Gas", "keywords": ["Gas Station"], "weight": 1.0, "is_must": true, "search_query": "ì£¼ìœ ì†Œ ë¦¬í„°ë‹¹ í• ì¸"}},
                {{"name": "Convenience", "keywords": ["Convenience Store"], "weight": 0.8, "is_must": false, "search_query": "í¸ì˜ì  í• ì¸"}}
            ]
        }}
        """
        
        msg = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        resp = LLMClient.call_api(msg)
        groups = resp.get("groups", [])
        safe_print("Step 2: Groups & Weights (with Must)", groups)
        return groups

# ============================================================
# 3. Hybrid Searcher (ES Boost & Nested Sum ì ìš©)
# ============================================================
class HybridSearcher:
    def __init__(self):
        self.es = Elasticsearch(ELASTICSEARCH_URL)
        self.embedder = embeddings 
    @traceable(run_type="retriever", name="Step3_Group_Search")
    def search_group(self, group: Dict) -> List[Dict]:
        """Step 3: ê° ê·¸ë£¹ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        query_text = group["search_query"]
        group_keywords = group["keywords"]
        
        # ğŸŒŸ [ë³€ê²½ 1] ê°€ì ¸ì˜¬ í•„ë“œì— 'previous_month_performance', 'domestic_year_cost' ì¶”ê°€
        target_fields = ["card_name", "card_id", "benefits.summary", "benefits.category", "previous_month_performance", "domestic_year_cost"]

        # --- 1. Vector Search ---
        try:
            vector = self.embedder.embed_query(query_text)
        except Exception as e:
            print(f"âš ï¸ Embedding API Error: {e}")
            vector = []

        vec_hits = []
        if vector and len(vector) > 0:
            try:
                vec_res = self.es.search(index=INDEX_NAME, knn={
                    "field": "benefits.vector",
                    "query_vector": vector,
                    "k": 50,
                    "num_candidates": 100
                }, _source=target_fields) # ğŸŒŸ target_fields ì‚¬ìš©
                vec_hits = vec_res["hits"]["hits"]
            except Exception as e:
                print(f"âš ï¸ Vector Search Error: {e}")

        # --- 2. Keyword Search ---
        should_clauses = []
        for kw in group_keywords:
            should_clauses.append({"match": {"benefits.summary": {"query": kw, "boost": 2.0}}})
            should_clauses.append({"match": {"benefits.category": {"query": kw, "boost": 1.0}}})

        key_hits = []
        try:
            key_res = self.es.search(index=INDEX_NAME, query={
                "nested": {
                    "path": "benefits",
                    "score_mode": "sum",
                    "query": {
                        "bool": {
                            "should": should_clauses,
                            "minimum_should_match": 1
                        }
                    },
                    "inner_hits": {
                        "_source": ["benefits.summary", "benefits.category", "benefits.value"],
                        "size": 3
                    }
                }
            }, size=50, _source=target_fields) # ğŸŒŸ target_fields ì‚¬ìš©
            key_hits = key_res["hits"]["hits"]
        except Exception as e:
            print(f"âš ï¸ Keyword Search Error: {e}")

        # --- 3. RRF Merge ---
        rrf_results = self._apply_rrf(vec_hits, key_hits)
        
        # ë©”íƒ€ë°ì´í„° ì£¼ì…
        for item in rrf_results:
            item["matched_group"] = group["name"]
            item["group_weight"] = group["weight"]
            item["is_must"] = group.get("is_must", False)
            item["search_keywords"] = group_keywords
        
        safe_print(f"Step 3: Search Results for '{group['name']}'", 
                   [f"{doc['card_name']} (RRF: {doc['rrf_score']:.4f})" for doc in rrf_results])
        
        return rrf_results

    def _apply_rrf(self, vec_hits, key_hits, k=60):
        scores = defaultdict(float)
        docs = {}
        
        # Vector Rank
        for rank, hit in enumerate(vec_hits):
            cid = hit["_id"]
            scores[cid] += 1 / (rank + k)
            docs[cid] = hit["_source"]
            docs[cid]["_id"] = cid
            docs[cid]["vec_score"] = hit["_score"]
            docs[cid]["key_score"] = 0.0
            docs[cid].setdefault("inner_hits_info", []) # ë²¡í„° ê²€ìƒ‰ì€ inner_hitsê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì—†ìŒ

        # Keyword Rank
        for rank, hit in enumerate(key_hits):
            cid = hit["_id"]
            scores[cid] += 1 / (rank + k)
            if cid not in docs:
                docs[cid] = hit["_source"]
                docs[cid]["_id"] = cid
                docs[cid]["vec_score"] = 0.0
            docs[cid]["key_score"] = hit["_score"]
            
            # Inner Hits ì €ì¥ (ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í˜œíƒ ì¶”ì¶œìš©)
            if "inner_hits" in hit:
                ih = hit["inner_hits"]["benefits"]["hits"]["hits"]
                docs[cid]["inner_hits_info"] = [h["_source"] for h in ih]
        
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        final_docs = []
        for cid in sorted_ids:
            d = docs[cid]
            d["rrf_score"] = scores[cid]
            final_docs.append(d)
            
        return final_docs

# ============================================================
# 4. Reranker (ëª¨ë“  ê´€ë ¨ í˜œíƒ ìˆ˜ì§‘ ë²„ì „)
# ============================================================
class Reranker:
    # ê¸°ì¡´ rerank_cards í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ í†µì§¸ë¡œ êµì²´í•˜ì„¸ìš”.

    @traceable(run_type="chain", name="Step4_Reranking")
    def rerank_cards(self, candidates: List[Dict], groups: List[Dict]) -> List[Dict]:
        """
        Step 4: Must Boost + Cross-Check + Diversity Bonus
        + [ë³€ê²½] JSON ì¶œë ¥ì„ ìœ„í•œ í•„ë“œ(ID, ì—°íšŒë¹„, ì „ì›”ì‹¤ì ) ë³´ì¡´
        """
        unique_cards = {}
        
        MUST_BOOST_FACTOR = 3.0       
        
        safe_print("Step 4", f"Reranking & Collecting ALL benefits...")

        # 1. ê¸°ë³¸ ì ìˆ˜ í•©ì‚° ë° ì´ˆê¸°í™”
        for cand in candidates:
            cid = cand.get("card_id", cand.get("card_name"))
            
            if cid not in unique_cards:
                unique_cards[cid] = {
                    "card_name": cand["card_name"],
                    "card_id": cand.get("card_id"), # ğŸŒŸ [ë³€ê²½] ID ì €ì¥
                    "previous_month_performance": cand.get("previous_month_performance", "ì •ë³´ì—†ìŒ"), # ğŸŒŸ [ë³€ê²½] ì „ì›”ì‹¤ì  ì €ì¥
                    "domestic_year_cost": cand.get("domestic_year_cost", "ì •ë³´ì—†ìŒ"), # ğŸŒŸ [ë³€ê²½] ì—°íšŒë¹„ ì €ì¥
                    "benefits": cand.get("benefits", []),
                    "inner_hits_info": cand.get("inner_hits_info", []),
                    "total_score": 0.0,
                    "matched_reasons": [],
                    "matched_summaries": set(),
                    "search_keywords": cand.get("search_keywords", []),
                    "matched_group_names": set()
                }
            
            g_weight = cand.get("group_weight", 1.0)
            is_must = cand.get("is_must", False)
            rrf_score = cand.get("rrf_score", 0.0)
            group_name = cand.get("matched_group", "General")
            
            if group_name in unique_cards[cid]["matched_group_names"]:
                continue

            base_score = rrf_score * 1000
            final_multiplier = g_weight * (MUST_BOOST_FACTOR if is_must else 1.0)
            score_contribution = base_score * final_multiplier
            
            unique_cards[cid]["total_score"] += score_contribution
            unique_cards[cid]["matched_group_names"].add(group_name)
            
            mark = "ğŸ”¥" if is_must else ""
            unique_cards[cid]["matched_reasons"].append(f"{mark}{group_name}({score_contribution:.1f})")
            
            if cand.get("inner_hits_info"):
                for hit in cand["inner_hits_info"]:
                    unique_cards[cid]["matched_summaries"].add(hit.get("summary", ""))

        # 2. Cross-Check (ë†“ì¹œ ê·¸ë£¹ ì°¾ê¸°) & ëª¨ë“  í˜œíƒ ìŠ¤ìº”
        for cid, card in unique_cards.items():
            for group in groups:
                g_name = group["name"]
                if g_name in card["matched_group_names"]: continue
                
                g_keywords = group["keywords"]
                g_weight = group["weight"]
                g_is_must = group.get("is_must", False)
                
                for ben in card["benefits"]:
                    text = (ben.get("summary", "") + " " + ben.get("category", "")).lower()
                    if any(k.lower() in text for k in g_keywords):
                        card["matched_group_names"].add(g_name)
                        bonus_score = 10.0 * g_weight * (MUST_BOOST_FACTOR if g_is_must else 1.0)
                        card["total_score"] += bonus_score
                        mark = "ğŸ”¥" if g_is_must else ""
                        card["matched_reasons"].append(f"{mark}{g_name}(Found! {bonus_score:.1f})")
                        break 
            
            active_keywords = []
            for group in groups:
                if group["name"] in card["matched_group_names"]:
                    active_keywords.extend(group["keywords"])
            
            for ben in card["benefits"]:
                text = (ben.get("summary", "") + " " + ben.get("category", "")).lower()
                if any(k.lower() in text for k in active_keywords):
                    card["matched_summaries"].add(ben.get("summary", ""))

        # 3. Diversity Bonus & ì •ë¦¬
        final_list = []
        for cid, card in unique_cards.items():
            match_count = len(card["matched_group_names"])
            diversity_multiplier = 1.0

            if match_count == 2: diversity_multiplier = 1.3
            elif match_count == 3: diversity_multiplier = 2.0
            elif match_count == 4: diversity_multiplier = 4.0
            elif match_count >= 5: diversity_multiplier = 10.0
            
            if diversity_multiplier > 1.0:
                card["total_score"] *= diversity_multiplier
                card["matched_reasons"].append(f"ğŸVariety(x{diversity_multiplier})")

            breakdown_text = " + ".join(card["matched_reasons"])
            
            all_benefits = sorted(list(card["matched_summaries"]))
            if not all_benefits and card["benefits"]:
                 all_benefits = [card["benefits"][0].get("summary", "")]

            # ğŸŒŸ [ë³€ê²½] ìµœì¢… ë°˜í™˜ í¬ë§·ì„ ìš”ì²­í•˜ì‹  í‚¤ê°’ìœ¼ë¡œ ë§ì¶¤
            final_list.append({
                "card_id": card["card_id"],
                "card_name": card["card_name"],
                "previous_month_performance": card["previous_month_performance"],
                "domestic_year_cost": card["domestic_year_cost"],
                "benefit_list": all_benefits,
                "match_reason": breakdown_text, # score_breakdown -> match_reasonìœ¼ë¡œ ë§¤í•‘
                "score": card["total_score"]
            })

        final_list.sort(key=lambda x: x["score"], reverse=True)
        return final_list[:3]

# ============================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# ============================================================
def run_pipeline(user_query: str):
    analyzer = QueryAnalyzer()
    searcher = HybridSearcher()
    reranker = Reranker()
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = analyzer.rewrite_and_extract(user_query)
    
    # 2. ê·¸ë£¹í•‘ ë° ê°€ì¤‘ì¹˜
    groups = analyzer.group_and_weight(user_query, keywords)
    
    # 3. ê·¸ë£¹ë³„ ê²€ìƒ‰ (ES Boost & Nested Sum)
    all_candidates = []
    for group in groups:
        group_results = searcher.search_group(group)
        all_candidates.extend(group_results)
        
    # 4. ë¦¬ë­í‚¹ (ë‹¨ìœ„ ê³„ì‚° ì—†ì´ ì ìˆ˜ í•©ì‚°)
    return reranker.rerank_cards(all_candidates, groups)

# ============================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€ (ì¶œë ¥ ë¶€ë¶„ ìˆ˜ì •)
# ============================================================
if __name__ == "__main__":
    # ... (ê¸°ì¡´ LangSmith ì„¤ì • ì½”ë“œ ë™ì¼) ...
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    if langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
        os.environ["LANGCHAIN_PROJECT"] = "CardBenefit ES_Score_Based"
    
    safe_print("ğŸ”", "Card Benefit Search (All Benefits Display)")
    print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥")
    
    while True:
        try:
            q = input("\nğŸ’¬ ì…ë ¥: ").strip()
        except KeyboardInterrupt: break
        if q.lower() in ["q", "exit"]: break
        if not q: continue

        start = time.perf_counter()
        results = run_pipeline(q)
        elapsed = time.perf_counter() - start

        print(f"\nâ±ï¸ Time: {elapsed:.4f}s")
        if results:
            print(f"ğŸ† [ì¶”ì²œ ê²°ê³¼ Top {len(results)}]")
            for i, res in enumerate(results):
                print(f"\n{i+1}. {res['card_name']} (Score: {res['score']:.1f})")
                print(f"   ğŸ“Š {res['score_breakdown']}")
                print(f"   ğŸ’¡ ê´€ë ¨ í˜œíƒ ëª¨ìŒ:")
                # ğŸŒŸ [ë³€ê²½] ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒë©´ì„œ ì „ë¶€ ì¶œë ¥
                for ben in res['benefit_list']:
                    print(f"      - {ben}")
        else:
            print("âš ï¸ ê²°ê³¼ ì—†ìŒ")