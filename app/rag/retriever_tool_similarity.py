import os
import sys
import requests
import json
from typing import List, Dict

# LangChain & LangSmith Imports
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from langchain_core.tools import tool
from langsmith import traceable
from dotenv import load_dotenv

load_dotenv()

ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
HF_API_KEY = os.getenv("HF_API_KEY")
INDEX_NAME = "card_benefit_bgem3_v1"

# ============================================================
# HF Router Chat Completion API Wrapper
# ============================================================

ROUTER_API_URL = "https://router.huggingface.co/v1/chat/completions"
HF_TOKEN = os.environ.get("HF_API_KEY") or os.environ.get("HF_TOKEN")

def hf_chat_completion(messages, model, max_tokens=120, temperature=0.2):
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }

    resp = requests.post(ROUTER_API_URL, headers=headers, json=payload)

    try:
        data = resp.json()
    except Exception as err:
        print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜:", err)
        print(resp.text)
        return None

    if "choices" not in data:
        print("âš ï¸ LLM ì‘ë‹µ ì˜¤ë¥˜:", data)
        return None

    return data["choices"][0]["message"]["content"]


# ============================================================
# Utility Safe Print
# ============================================================

def safe_print(text):
    try:
        print(text.encode("utf-8", "ignore").decode("utf-8"))
    except:
        print(text)


# ============================================================
# ë¸Œëœë“œ ì‚¬ì „ & ì‚¬ìš©ì ì…ë ¥ ë¸Œëœë“œ ì¶”ì¶œ
# ============================================================

# ì „ì²´ ë¸Œëœë“œ ì‚¬ì „ (ì—¬ê¸°ì„œëŠ” ì£¼ìš” ì˜ˆì‹œë§Œ ë„£ì—ˆì§€ë§Œ, ë‚˜ì¤‘ì— ë” ì¶”ê°€í•˜ë©´ ë¨)
ALL_BRANDS = [
    "ìŠ¤íƒ€ë²…ìŠ¤", "ìŠ¤ë²…",
    "íˆ¬ì¸", "íˆ¬ì¸í”Œë ˆì´ìŠ¤",
    "ì´ë””ì•¼",
    "ì»¤í”¼ë¹ˆ", "í• ë¦¬ìŠ¤", "í´ë°”ì…‹",
    "ë¹½ë‹¤ë°©", "ë©”ê°€ì»¤í”¼", "ì—”ì œë¦¬ë„ˆìŠ¤",
]

def extract_user_brands(original_query: str) -> List[str]:
    """ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ì—ì„œ ë“±ì¥í•œ ë¸Œëœë“œë§Œ ì¶”ì¶œ"""
    found = []
    for b in ALL_BRANDS:
        if b in original_query:
            found.append(b)
    # ì¤‘ë³µ ì œê±° ìˆœì„œ ìœ ì§€
    return list(dict.fromkeys(found))


# ============================================================
# Query Expansion using HF ChatCompletion API
# ============================================================

@traceable(name="0. Expand Queries (HF ChatCompletion API)", run_type="llm")
def generate_expanded_queries(original_query: str) -> List[str]:
    """ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ìš© í™•ì¥ í‚¤ì›Œë“œ ìƒì„±"""

    if not HF_TOKEN:
        safe_print("âŒ HF Token ì—†ìŒ â€” ì›ë³¸ ì‚¬ìš©")
        return [original_query]

    MODEL_NAME = "google/gemma-2-9b-it:nebius"
    # MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct:novita"

    messages = [
        {
            "role": "system",
            "content": "ë„ˆëŠ” í•œêµ­ì–´ ê²€ìƒ‰ ì§ˆì˜ í™•ì¥ ì „ë¬¸ê°€ì•¼."
        },
        {
            "role": "user",
            "content": f"""
ë‹¤ìŒ ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ìš© í™•ì¥ í‚¤ì›Œë“œ 3ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.

ê·œì¹™:
- ì¹´í˜/ì»¤í”¼ ë¸Œëœë“œ ì¤„ì„ë§ í™•ì¥ (ìŠ¤ë²…â†’ìŠ¤íƒ€ë²…ìŠ¤ ë“±)
- ì˜¤íƒˆì ë³´ì •
- ëª…ì‚¬êµ¬ í˜•íƒœë¡œ
- ì¶œë ¥ì€ "í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"

ë¬¸ì¥: {original_query}
"""
        }
    ]

    response_text = hf_chat_completion(messages, model=MODEL_NAME)
    if not response_text:
        return [original_query]

    keywords = [k.strip() for k in response_text.split(",") if k.strip()]

    # ì¹´í˜ ê´€ë ¨ ì¼ë°˜ í‚¤ì›Œë“œëŠ” "ê²€ìƒ‰ ë³´ì¡°ìš©"ìœ¼ë¡œë§Œ ì¶”ê°€
    keywords += ["ì¹´í˜ í• ì¸", "ì»¤í”¼ í• ì¸", "ë¸Œëœë“œ ì¹´í˜ í˜œíƒ"]

    safe_print(f"ğŸ”€ í™•ì¥ëœ í‚¤ì›Œë“œ: {keywords}")
    return keywords


# ============================================================
# Retrieval Helpers (ìœ ì‚¬ë„ ìŠ¤ì½”ì–´ í•„í„°ë§ + ê°€ì¤‘ì¹˜)
# ============================================================

def calculate_benefit_score(doc, user_brands: List[str]) -> float:
    """í˜œíƒ ê°’ + ì‚¬ìš©ì ë¸Œëœë“œ + ì¹´í˜ ê´€ë ¨ ì—¬ë¶€ë¥¼ ì¢…í•©í•´ì„œ ì ìˆ˜ ê³„ì‚°"""
    try:
        base = float(doc.metadata.get("benefit_value", 0))
        unit = doc.metadata.get("benefit_unit", "NONE")
        category = doc.metadata.get("category", "")
        summary = doc.metadata.get("benefit_summary", "")

        # ê¸°ë³¸ ì ìˆ˜: %ë©´ ê·¸ëŒ€ë¡œ, ìˆ«ìë©´ ê·¸ëŒ€ë¡œ (ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ë” íŠœë‹)
        if unit == "%":
            score = base
        else:
            score = base

        text_all = (doc.page_content or "") + " " + summary

        # â‘  ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¸Œëœë“œê°€ ë¬¸ì„œì— ë“±ì¥í•˜ë©´ ê°•í•œ ê°€ì¤‘ì¹˜
        if user_brands and any(b in text_all for b in user_brands):
            score *= 3

        # â‘¡ ì¼ë°˜ ì¹´í˜/ì»¤í”¼ ê´€ë ¨ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ìˆìœ¼ë©´ ë³´ì¡° ê°€ì¤‘ì¹˜
        if any(x in text_all for x in ["ì»¤í”¼", "ì¹´í˜"]):
            score *= 2

        # â‘¢ ê¸ˆìœµ/í™˜ì „/ATM ìœ„ì£¼ì˜ í˜œíƒì€ ì´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì¤‘ìš”ë„ ë‚®ê²Œ
        if "ê¸ˆìœµ" in category or "í™˜ì „" in summary or "ATM" in summary:
            score *= 0.2

        return score

    except:
        return 0.0


@traceable(name="1. Retrieve Candidates (ES)", run_type="retriever")
def retrieve_candidates(vector_store, queries: List[str], k_per_query: int):

    all_docs = []

    for q in queries:
        if not q.strip():
            continue

        # ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨ ê²€ìƒ‰
        docs_with_scores = vector_store.similarity_search_with_score(q, k=k_per_query)

        for doc, score in docs_with_scores:
            # ğŸ’¡ ìœ ì‚¬ë„ Threshold ì ìš© (ë„ˆ ìƒí™©ì— ë§ê²Œ íŠœë‹ ê°€ëŠ¥)
            if score < 0.15:
                continue
            all_docs.append(doc)

    # page_content ê¸°ì¤€ dedup
    unique_docs = {doc.page_content: doc for doc in all_docs}.values()
    return list(unique_docs)


# ============================================================
# Rerank Candidates
# ============================================================

@traceable(name="2. Rerank Candidates", run_type="parser")
def rerank_candidates(docs: List, user_brands: List[str]) -> List:
    """ì‚¬ìš©ì ë¸Œëœë“œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ í›„ë³´êµ° ì •ë ¬"""
    docs_scored = []
    for doc in docs:
        score = calculate_benefit_score(doc, user_brands)
        docs_scored.append((doc, score))

    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
    sorted_docs = sorted(docs_scored, key=lambda x: x[1], reverse=True)
    return [d for d, s in sorted_docs]


# ============================================================
# Select Final Results
# ============================================================

@traceable(name="3. Select Top Results", run_type="parser")
def select_final_results(sorted_docs: List, top_k: int, user_brands: List[str]) -> List[Dict]:

    top_results = []
    seen = set()

    for doc in sorted_docs:
        card_name = doc.metadata.get("card_name", "")
        if card_name in seen:
            continue

        seen.add(card_name)

        final_score = calculate_benefit_score(doc, user_brands)

        top_results.append({
            "card_name": card_name,
            "origin_id": doc.metadata.get("origin_id", ""),
            "benefit_summary": doc.metadata.get("benefit_summary", ""),
            "benefit_value": doc.metadata.get("benefit_value", ""),
            "benefit_unit": doc.metadata.get("benefit_unit", ""),
            "score": final_score,
            "category": doc.metadata.get("category", ""),
            "detail": doc.page_content
        })

        if len(top_results) >= top_k:
            break

    return top_results


# ============================================================
# Main Tool
# ============================================================

@tool
def retriever_tool(query: str) -> List[Dict]:
    """Search for credit card benefits related to the query using Hugging Face API."""
    try:
        if not HF_API_KEY:
            safe_print("âŒ HF_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        embeddings = HuggingFaceEndpointEmbeddings(
            model="BAAI/bge-m3",
            task="feature-extraction",
            huggingfacehub_api_token=HF_API_KEY,
        )

        vector_store = ElasticsearchStore(
            es_url=ELASTICSEARCH_URL,
            index_name=INDEX_NAME,
            embedding=embeddings
        )

        # 0) ì‚¬ìš©ì ì…ë ¥ ë¸Œëœë“œ ì¶”ì¶œ (ì—¬ê¸°ì—ë§Œ ê°€ì¤‘ì¹˜ ì ìš©)
        user_brands = extract_user_brands(query)
        safe_print(f"ğŸ‘€ ì‚¬ìš©ì ì…ë ¥ ë¸Œëœë“œ: {user_brands}")

        # 1) ì¿¼ë¦¬ í™•ì¥
        expanded_queries = generate_expanded_queries(query)

        # 2) ê²€ìƒ‰
        candidate_docs = retrieve_candidates(vector_store, expanded_queries, k_per_query=20)

        # 3) ë¦¬ë­í‚¹ (ì‚¬ìš©ì ë¸Œëœë“œ ê°€ì¤‘ì¹˜ ë°˜ì˜)
        ranked_docs = rerank_candidates(candidate_docs, user_brands)

        # 4) ìµœì¢… Top-k ì„ íƒ
        final_results = select_final_results(ranked_docs, top_k=3, user_brands=user_brands)

        return final_results

    except Exception as e:
        safe_print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


# ============================================================
# Test Block (LangSmith í¬í•¨)
# ============================================================

if __name__ == "__main__":
    # LangSmith Tracing ì„¤ì •
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    if langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
        os.environ["LANGCHAIN_PROJECT"] = "CardBenefit RAG Debug"
        safe_print(f"âœ… LangSmith Tracing Enabled (Project: {os.environ['LANGCHAIN_PROJECT']})")
    else:
        safe_print("âš ï¸ LangSmith API Key not found. Tracing disabled.")

    safe_print("ğŸ” API ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Query Expansion: HF Router ChatCompletion)")
    safe_print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    while True:
        try:
            try:
                user_query = input("ğŸ’¬ ê²€ìƒ‰ì–´ ì…ë ¥: ").strip()
            except UnicodeDecodeError:
                continue

            if user_query.lower() in ["q", "quit", "exit"]:
                safe_print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_query:
                continue

            safe_print(f"\nğŸš€ '{user_query}' ì²˜ë¦¬ ì¤‘...\n")

            results = retriever_tool.invoke(user_query)

            if results:
                safe_print(f"\nğŸ† [ìµœì¢… ì¶”ì²œ Top {len(results)}]")
                safe_print("=" * 50)
                for i, res in enumerate(results):
                    score_info = f"(ì ìˆ˜: {res.get('score', 0):.1f})"
                    card_info = f"âœ… {i+1}ìœ„ [ID:{res.get('origin_id')}]: {res.get('card_name')}"
                    benefit_info = f"   í˜œíƒ: {res.get('benefit_summary')}"
                    detail_info = f"   ìƒì„¸: {score_info}"

                    safe_print(card_info)
                    safe_print(benefit_info)
                    safe_print(detail_info)
                    safe_print("-" * 50)
            else:
                safe_print("âš ï¸ ê²°ê³¼ ì—†ìŒ\n")

        except KeyboardInterrupt:
            safe_print("\nğŸ‘‹ ê°•ì œ ì¢…ë£Œ")
            break
        except Exception as e:
            safe_print(f"âŒ ì˜¤ë¥˜: {e}\n")
