import json
import os
import sys
import time
import argparse
from tqdm import tqdm

# ------------------------------------------------------------
# [ì„¤ì •] rag ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
# ------------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))          # .../rag
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, ".."))  # .../app
sys.path.append(PROJECT_ROOT)

from rag.chatbot_pipeline import run_pipeline


# ------------------------------------------------------------
# [ê¸°ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ]
# 1) ë°œí‘œìš© í†µí•©ì…‹ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
# 2) ì—†ìœ¼ë©´ ê¸°ì¡´ evaluation_dataset.json ì‚¬ìš©
# ------------------------------------------------------------
DEFAULT_MIXED = os.path.join(BASE_DIR, "data", "evaluation_mixed_for_presentation.json")
DEFAULT_LEGACY = os.path.join(BASE_DIR, "data", "evaluation_dataset.json")

def resolve_default_dataset():
    if os.path.exists(DEFAULT_MIXED):
        return DEFAULT_MIXED
    return DEFAULT_LEGACY


# ------------------------------------------------------------
# Metrics
# ------------------------------------------------------------
def calculate_metrics(dataset_path: str, k: int = 3, verbose: bool = False):
    if not os.path.exists(dataset_path):
        print(f"âŒ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        print("   rag/data/ ì•„ë˜ì— JSONì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    with open(dataset_path, "r", encoding="utf-8") as f:
        eval_data = json.load(f)

    if not isinstance(eval_data, list) or len(eval_data) == 0:
        print("âŒ í‰ê°€ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (list of dict í•„ìš”)")
        return

    print(f"ğŸš€ RAG ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"   Dataset: {dataset_path}")
    print(f"   Top-K  : {k}")
    print(f"   Total  : {len(eval_data)}")
    print("-" * 70)

    if verbose:
        print(f"{'Type':<10} | {'Query':<45} | {'Result':<10} | {'Rank'}")
        print("-" * 70)

    total_count = 0
    hit_count = 0
    mrr_sum = 0.0

    # íƒ€ì…ë³„ í†µê³„
    type_stats = {}

    # source_setì´ ìˆìœ¼ë©´ ì„¸íŠ¸ë³„ í†µê³„ë„
    has_source = any(isinstance(item, dict) and "source_set" in item for item in eval_data)
    source_stats = {}

    start_all = time.perf_counter()

    for item in tqdm(eval_data):
        if not isinstance(item, dict):
            continue

        query = item.get("query", "").strip()
        if not query:
            continue

        q_type = item.get("type", "Complex")
        ground_truth_ids = item.get("ground_truth_ids") or item.get("ground_truth") or []
        ground_truth_ids = set(map(str, ground_truth_ids))

        source_set = item.get("source_set", "Unknown") if has_source else None

        # init stats buckets
        if q_type not in type_stats:
            type_stats[q_type] = {"hit": 0, "total": 0}

        if has_source:
            if source_set not in source_stats:
                source_stats[source_set] = {"hit": 0, "total": 0}

        try:
            results = run_pipeline(query) or []
            recommended_ids = [str(r.get("card_id") or r.get("id")) for r in results[:k]]

            is_hit = False
            rank = 0

            for idx, rec_id in enumerate(recommended_ids):
                if rec_id in ground_truth_ids:
                    is_hit = True
                    rank = idx + 1
                    break

            # update global
            total_count += 1
            type_stats[q_type]["total"] += 1
            if has_source:
                source_stats[source_set]["total"] += 1

            if is_hit:
                hit_count += 1
                mrr_sum += 1.0 / rank
                type_stats[q_type]["hit"] += 1
                if has_source:
                    source_stats[source_set]["hit"] += 1

                if verbose:
                    print(f"{q_type:<10} | {query[:43]:<45} | âœ… HIT     | {rank}")
            else:
                if verbose:
                    print(f"{q_type:<10} | {query[:43]:<45} | âŒ MISS    | -")

        except Exception as e:
            # ì—ëŸ¬ë„ totalë¡œ í¬í•¨í• ì§€ ì—¬ë¶€ëŠ” ì •ì±…ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ
            # ì§€ê¸ˆì€ "í‰ê°€ ì‹¤íŒ¨"ë¡œë§Œ ë¡œê·¸ ë‚¨ê¸°ê³  ìŠ¤í‚µ
            if verbose:
                print(f"{q_type:<10} | {query[:43]:<45} | âš ï¸ ERROR  | -")
            print(f"Error processing query '{query}': {e}")

    elapsed_all = time.perf_counter() - start_all

    # --- Final Report ---
    hit_rate = (hit_count / total_count) * 100 if total_count > 0 else 0.0
    mrr = (mrr_sum / total_count) if total_count > 0 else 0.0

    print("\n" + "=" * 50)
    print("ğŸ“Š [Final Evaluation Report]")
    print("=" * 50)
    print(f"ğŸ¯ Overall Hit Rate @ {k} : {hit_rate:.2f}% ({hit_count}/{total_count})")
    print(f"ğŸ¥‡ Overall MRR            : {mrr:.4f}")
    print(f"â±ï¸ Total Time             : {elapsed_all:.2f}s")
    print("-" * 50)

    # íƒ€ì…ë³„ ìƒì„¸ ê²°ê³¼
    for t, stat in type_stats.items():
        t_hit_rate = (stat["hit"] / stat["total"] * 100) if stat["total"] > 0 else 0.0
        print(f"ğŸ”¹ {t:<10} Hit Rate    : {t_hit_rate:.2f}% ({stat['hit']}/{stat['total']})")

    # source_setì´ ìˆìœ¼ë©´ ì„¸íŠ¸ë³„ë„ ì¶œë ¥
    if has_source:
        print("-" * 50)
        print("ğŸ“š [By Source Set]")
        for s, stat in source_stats.items():
            s_hit_rate = (stat["hit"] / stat["total"] * 100) if stat["total"] > 0 else 0.0
            print(f"ğŸ”¸ {s:<18} Hit Rate : {s_hit_rate:.2f}% ({stat['hit']}/{stat['total']})")

    print("=" * 50)

    # ë°œí‘œ ìë£Œìš© í…ìŠ¤íŠ¸
    print("\n[ğŸ“¢ ë°œí‘œ ìë£Œìš© ìš”ì•½ ë©˜íŠ¸]")
    if total_count == 0:
        print("\"í‰ê°€ ë°ì´í„°ê°€ ì—†ì–´ ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"")
        return

    avg_rank = (1 / mrr) if mrr > 0 else float("inf")
    avg_rank_text = f"{avg_rank:.1f}" if mrr > 0 else "N/A"

    print(
        f"\"ì´ {total_count}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬(ë‹¨ìˆœ/ë³µí•© í˜¼í•©)ì— ëŒ€í•´ í‰ê°€ë¥¼ ì§„í–‰í•œ ê²°ê³¼,\n"
        f"ìƒìœ„ {k}ê°œ ì¶”ì²œ ë‚´ ì •ë‹µ í¬í•¨ ë¹„ìœ¨ì¸ Hit RateëŠ” {hit_rate:.1f}%ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°,\n"
        f"í‰ê· ì ìœ¼ë¡œ ì •ë‹µ ì¹´ë“œê°€ {avg_rank_text}ë²ˆì§¸ ìˆœìœ„ì— ë…¸ì¶œë˜ëŠ” {mrr:.2f}ì˜ MRR ì ìˆ˜ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\""
    )


# ------------------------------------------------------------
# CLI
# ------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Evaluate RAG performance for card recommendation.")
    parser.add_argument("-k", type=int, default=3, help="Top-K for Hit Rate and MRR.")
    parser.add_argument(
        "--dataset",
        type=str,
        default=resolve_default_dataset(),
        help="Path to evaluation dataset JSON."
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print per-query results."
    )

    args = parser.parse_args()
    calculate_metrics(dataset_path=args.dataset, k=args.k, verbose=args.verbose)


if __name__ == "__main__":
    main()

# ë°œí‘œìš© í†µí•©ì…‹ì„ ì´ë¯¸ rag/data/ì— ë„£ì—ˆë‹¤ë©´:

# cd /app/rag
# python evaluate_performance.py


# íŠ¹ì • íŒŒì¼ë¡œ ëŒë¦¬ê³  ì‹¶ìœ¼ë©´:

# python evaluate_performance.py --dataset data/evaluation_humanstyle_complex.json
# python evaluate_performance.py --dataset data/evaluation_synonym_variations.json
# python evaluate_performance.py --dataset data/evaluation_hard_negative.json


# ë¡œê·¸ê¹Œì§€ ë³´ê³  ì‹¶ìœ¼ë©´:

# python evaluate_performance.py --verbose